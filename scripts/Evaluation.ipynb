{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/alexmoed/Master_project_Sonata/blob/Eval/eval_v006.ipynb",
      "authorship_tag": "ABX9TyOsq9ah6D/o1EYLiwtkUKVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmoed/MastersProject_Submission/blob/main/scripts/Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If using colab mount drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "n9GX8cbSuoZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee803ca-5595-4fe1-97ec-30fa6a63df54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " @brief ScanNet validation inference script with SONATA+PointGroup\n",
        " Organization and export logic assisted by Claude AI (Anthropic)\n",
        " Multiple prompts used for code structure, error handling, statistics tracking,\n",
        " and data processing pipeline organization (abbreviated from extended conversation).\n",
        " Base training framework and tools from: Pointcept Contributors (2023). Pointcept: A Codebase for Point Cloud Perception Research [online]. [Accessed 2025]. Available from: \"https://github.com/Pointcept/Pointcept\".# Original Author: Xiaoyang Wu (xiaoyang.wu.cs@gmail.com)"
      ],
      "metadata": {
        "id": "xYlKe2mCK6QB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLS"
      ],
      "metadata": {
        "id": "KI7vU5-KBUh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy versions are a ongoing issue in this install it must be below 1.25 to work 1.24.and 1.24.3 are tested and work\n",
        "#Uninstall numpy first\n",
        "#Restart session!!\n",
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y numpy  # Run twice\n",
        "\n",
        "# Find any installs of numpy left\n",
        "!find /usr/local/lib/python*/site-packages -name \"numpy*\" -type d\n",
        "\n",
        "# Delete ALL numpy folders\n",
        "!rm -rf /usr/local/lib/python*/site-packages/numpy*\n",
        "!rm -rf /usr/local/lib/python*/dist-packages/numpy*\n",
        "!rm -rf ~/.local/lib/python*/site-packages/numpy*\n",
        "\n",
        "# Clean pip cache too\n",
        "!pip cache purge\n",
        "\n",
        "\n",
        "!pip install numpy==1.24.0 --no-cache-dir\n",
        "\n",
        "# Make sure the versions are correct and reload if not it needs to be 1.24.0\n",
        "import numpy as np\n",
        "if np.__version__ != \"1.24.0\":\n",
        "   print(f\"Wrong version: {np.__version__} Please restart runtime and rerun\")\n",
        "\n",
        "\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Numpy location: {np.__file__}\")\n"
      ],
      "metadata": {
        "id": "EiJR_jKXstJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "abcf46d3-05dd-416f-d76b-17d6542325b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.24.0\n",
            "Uninstalling numpy-1.24.0:\n",
            "  Successfully uninstalled numpy-1.24.0\n",
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mfind: ‘/usr/local/lib/python*/site-packages’: No such file or directory\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Collecting numpy==1.24.0\n",
            "  Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m315.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.90 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\n",
            "treescope 0.1.10 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "scipy 1.16.1 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "pywavelets 1.9.0 requires numpy<3,>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "87da61aa181145d1ad2643dd55218898"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version: 1.24.0\n",
            "Numpy location: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import universal dependencies that arent impacted by order\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TA6820kf-QIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a verison cuda install that after running once that it saves the download and caches the install for speed. Also handles torch installs\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j8OAZS2nBX_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first batch of installs installing cuda and pytorch older versions: This will take 10+ minutes\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_cuda_torch.py').read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qEd_fRE1fy",
        "outputId": "be7157fb-ab47-4fba-f39d-b4c029c80a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "============================================================\n",
            "CUDA INSTALLATION\n",
            "============================================================\n",
            "\n",
            "1. Cleaning existing CUDA installations...\n",
            "✓ Cleaned\n",
            "\n",
            "2. Installing CUDA keyring...\n",
            "✓ Using cached keyring\n",
            "✓ Keyring installed\n",
            "\n",
            "3. Installing CUDA Toolkit 11.8...\n",
            "✓ CUDA 11.8 installed\n",
            "\n",
            "4. Setting environment variables...\n",
            "✓ Environment set\n",
            "\n",
            "5. Verifying installation...\n",
            "\n",
            "✅ CUDA installation complete!\n",
            "Cached files in: /content/drive/MyDrive/colab_cache\n",
            "\n",
            "============================================================\n",
            "PYTORCH INSTALLATION\n",
            "============================================================\n",
            "PyTorch: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "CUDA version: 11.8\n",
            "NumPy: 1.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pointops that is needed for Sonata decoder head. This you have to run twice\n",
        "\n",
        "%cd Pointcept/libs/pointops\n",
        "!pip install -v -e.\n",
        "%cd /content/drive/MyDrive/Pointcept\n"
      ],
      "metadata": {
        "id": "UB-Vs7X6zcDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f9f1a4-6d97-4862-8ab2-5a1e0c2808ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Obtaining file:///content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/SOURCES.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-4cqj4mfj/pointops.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (1.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2025.7.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pointops==1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->pointops==1.0) (1.3.0)\n",
            "Installing collected packages: pointops\n",
            "  Running setup.py develop for pointops\n",
            "    Running command python setup.py develop\n",
            "    running develop\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``easy_install``.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      easy_install.initialize_options(self)\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` directly.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    running egg_info\n",
            "    writing pointops.egg-info/PKG-INFO\n",
            "    writing dependency_links to pointops.egg-info/dependency_links.txt\n",
            "    writing requirements to pointops.egg-info/requires.txt\n",
            "    writing top-level names to pointops.egg-info/top_level.txt\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    reading manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "      warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "    Creating /usr/local/lib/python3.11/dist-packages/pointops.egg-link (link to .)\n",
            "    Adding pointops 1.0 to easy-install.pth file\n",
            "\n",
            "    Installed /content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Successfully installed pointops-1.0\n",
            "/content/drive/MyDrive/Pointcept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #This install mostly installs depencencies having to do with pointops\n",
        " %cd /content/drive/MyDrive/Pointcept/Installs\n",
        " exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_build_env.py').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihPYki8jV9Ec",
        "outputId": "ddfd3d8a-bfe3-4f78-d1df-a005c914fd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "2.1.0+cu118\n",
            "NumPy: 1.24.0\n",
            "NumPy: 1.24.0\n",
            "CHECKING ENVIRONMENT\n",
            "✓ GPU Available: NVIDIA A100-SXM4-40GB\n",
            "✓ CUDA Version: 11.8\n",
            "✓ PyTorch Version: 2.1.0+cu118\n",
            "\n",
            "✓ CUDA Version for installations: cu118\n",
            "FIXING NUMPY COMPATIBILITY\n",
            "✓ NumPy 1.24.0 compatibility fixes applied\n",
            "INSTALLING DEPENDENCIES\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:89: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:89: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:89: FutureWarning: In the future `np.str` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing basic dependencies...\n",
            "\n",
            "Installing PyTorch Geometric dependencies...\n",
            "Installing torch-scatter...\n",
            "Installing torch-sparse...\n",
            "Installing torch-cluster...\n",
            "Installing torch-geometric...\n",
            "\n",
            "Installing spconv...\n",
            "CLONING POINTCEPT\n",
            "✓ Cloned Pointcept\n",
            "✓ GPU: NVIDIA A100-SXM4-40GB, Using CUDA architecture: 8.0\n",
            "BUILDING POINTOPS\n",
            "Building pointops with pip...\n",
            "✓ pointops imported successfully\n",
            "\n",
            "============================================================\n",
            "BUILDING POINTGROUP_OPS\n",
            "============================================================\n",
            "Building pointgroup_ops with pip...\n",
            "✓ pointgroup_ops imported successfully\n",
            "\n",
            "============================================================\n",
            "FINAL VERIFICATION\n",
            "============================================================\n",
            "✓ torch (2.1.0+cu118)\n",
            "✓ numpy (1.24.0)\n",
            "✓ sklearn (1.3.2)\n",
            "✓ torch_cluster (installed)\n",
            "✓ pointops (installed)\n",
            "✓ pointgroup_ops (installed)\n",
            "✓ spconv (2.3.8)\n",
            "\n",
            "✅ Installation completed successfully!\n",
            "\n",
            "To use Pointcept, add these lines at the start of your code:\n",
            "```python\n",
            "import sys\n",
            "sys.path.insert(0, '/content/Pointcept')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointops')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointgroup_ops')\n",
            "```\n",
            "\n",
            "============================================================\n",
            "QUICK FUNCTIONALITY TEST\n",
            "============================================================\n",
            "✓ Created test tensor on GPU\n",
            "✓ Basic functionality test passed!\n",
            "pointops._C imported successfully!\n",
            "torch-scatter imported successfully!\n",
            "Numpy: 1.24.0\n",
            "Has np.bool: True\n",
            "✅ torch_cluster imported!\n",
            "NumPy version: 1.24.0\n",
            "Patch applied - np.bool is now: <class 'numpy.bool_'>\n",
            "Ops loaded: <built-in method ballquery_batch_p of PyCapsule object at 0x79feb1116550> <built-in method bfs_cluster of PyCapsule object at 0x79feb1116490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and impliment pointops patches and wrapper that fixes ballqueary errors in pointops\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/fix_ballquery_wrapper.py').read())"
      ],
      "metadata": {
        "id": "pTYh7JZv2SE4",
        "outputId": "0c980289-580f-47df-8ff7-53ceaa5bcc52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "FIXING BALLQUERY WRAPPER AFTER TORCH_GEOMETRIC INSTALL\n",
            "================================================================================\n",
            "\n",
            "1. CHECKING ENVIRONMENT AND DEPENDENCIES\n",
            "----------------------------------------\n",
            "  Python version: 3.11.13\n",
            "  ✓ Python version OK\n",
            "  PyTorch version: 2.1.0+cu118\n",
            "  CUDA available: True\n",
            "  CUDA version: 11.8\n",
            "  GPU: NVIDIA A100-SXM4-40GB\n",
            "  PyTorch built with CUDA: 11.8\n",
            "\n",
            "  Checking core dependencies:\n",
            "  ✓ numpy\n",
            "  ✓ plotly\n",
            "  ✓ trimesh\n",
            "  ✓ psutil\n",
            "\n",
            "  ✓ Pointcept found at: /content/Pointcept\n",
            "\n",
            "  Checking pointgroup_ops:\n",
            "  ✓ pointgroup_ops found at: /content/drive/MyDrive/Pointcept/libs/pointgroup_ops/pointgroup_ops.cpython-311-x86_64-linux-gnu.so\n",
            "  ⚠ WARNING: Loading .so file directly\n",
            "  ✓ Has ballquery_batch_p: <class 'builtin_function_or_method'>\n",
            "  ✓ pointgroup_ops_cuda found at: /usr/local/lib/python3.11/dist-packages/pointgroup_ops_cuda.cpython-311-x86_64-linux-gnu.so\n",
            "  ✓ pointops installed (optional)\n",
            "\n",
            "\n",
            "2. FIXING MODEL IMPORTS\n",
            "----------------------------------------\n",
            "\n",
            "  Processing point_group_v1m2_custom_criteria.py...\n",
            "    Created backup: point_group_v1m2_custom_criteria.py.backup_20250815_124459\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "  Processing point_group_v1m1_base.py...\n",
            "    Created backup: point_group_v1m1_base.py.backup_20250815_124459\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "\n",
            "3. FIXING UTILS.PY WRAPPER\n",
            "----------------------------------------\n",
            "  Created backup: utils.py.backup_20250815_124459\n",
            "  Current state:\n",
            "    Has BallQueryBatchP class: True\n",
            "    Has wrapper assignment: True\n",
            "    Has wrapper function: False\n",
            "  ✓ Fixed utils.py wrapper\n",
            "\n",
            "\n",
            "4. CLEARING ALL CACHES\n",
            "----------------------------------------\n",
            "  ✓ Cleared 12 loaded modules from memory\n",
            "\n",
            "\n",
            "5. APPLYING TORCH TYPE COMPATIBILITY FIXES\n",
            "----------------------------------------\n",
            "  ✓ Fixed torch.uint16\n",
            "  ✓ Fixed torch.uint32\n",
            "  ✓ Fixed torch.uint64\n",
            "\n",
            "\n",
            "============================================================\n",
            "FIX COMPLETE\n",
            "============================================================\n",
            "✓ Fixes applied: 5\n",
            "⚠ Warnings: 1\n",
            "  - pointgroup_ops is loading .so directly, not the Python wrapper\n",
            "\n",
            "✅ Ready to import Pointcept and run inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following runs predictions using Sonta and pointgroup and saves the predictions are NPZ files."
      ],
      "metadata": {
        "id": "54LGRqUGYTLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your config and checkpoint paths here:\n",
        "#Output path:\n",
        "SCANNET_VAL_PATH = '/content/drive/MyDrive/Scannet_dataset/val'\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/Pointcept/scannet_val_evaluation_v1_epoch795_v02' #best epoch 465\n",
        "#Checkpoint here:\n",
        "checkpoint_path = '/content/drive/MyDrive/Pointcept/exp/scannet/insseg-pointgroup-sonata_config_4/model/epoch_795.pth'\n",
        "#Change config file here:\n",
        "cfg = Config.fromfile('/content/drive/MyDrive/Pointcept/Pointcept/configs/scannet/insseg-pointgroup-v1m2-0-ptv3-base_2.py')\n",
        "\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_PATH, 'predictions'), exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RUN INFERENCE AND SAVE PREDICTIONS WITH POINTGROUP + SONATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nLoading SONATA+PointGroup model...\")\n",
        "model = build_model(cfg.model).cuda()\n",
        "model.eval()\n",
        "\n",
        "#Loading checkpoint here\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "weight = OrderedDict()\n",
        "for key, value in checkpoint[\"state_dict\"].items():\n",
        "    if key.startswith(\"module.\"):\n",
        "        key = key[7:]\n",
        "    weight[key] = value\n",
        "model.load_state_dict(weight, strict=True)\n",
        "print(\"Model loaded\")\n",
        "\n",
        "\n",
        "transform_list = [\n",
        "    dict(type=\"CenterShift\", apply_z=True),\n",
        "    dict(\n",
        "        type=\"Copy\",\n",
        "        keys_dict={\n",
        "            \"coord\": \"origin_coord\",\n",
        "            \"segment\": \"origin_segment\",\n",
        "            \"instance\": \"origin_instance\",\n",
        "        },\n",
        "    ),\n",
        "    dict(\n",
        "        type=\"GridSample\",\n",
        "        grid_size=0.0175, #Experiment with different gridsizes usually from .01 - .025  smaller the size the more detail and vram useage\n",
        "        hash_type=\"fnv\",\n",
        "        mode=\"train\",\n",
        "        return_grid_coord=True,\n",
        "    ),\n",
        "    dict(type=\"CenterShift\", apply_z=False),\n",
        "    dict(type=\"NormalizeColor\"),\n",
        "    dict(\n",
        "        type=\"InstanceParser\",\n",
        "        segment_ignore_index=(-1, 0, 1),\n",
        "        instance_ignore_index=-1,\n",
        "    ),\n",
        "]\n",
        "transform = Compose(transform_list)\n",
        "\n",
        "#Load validation scenes\n",
        "val_scenes = sorted([d for d in os.listdir(SCANNET_VAL_PATH) if os.path.isdir(os.path.join(SCANNET_VAL_PATH, d))])\n",
        "print(f\"\\nProcessing {len(val_scenes)} validation scenes...\")\n",
        "\n",
        "# Track stats\n",
        "stats = {\n",
        "    'processed': 0,\n",
        "    'failed': 0,\n",
        "    'total_predictions': 0,\n",
        "    'total_gt_instances': 0\n",
        "}\n",
        "\n",
        "# Process each scene in folder\n",
        "for scene_name in tqdm(val_scenes, desc=\"Running inference\"):\n",
        "    scene_path = os.path.join(SCANNET_VAL_PATH, scene_name)\n",
        "\n",
        "    try:\n",
        "        #Load scene data\n",
        "        coord = np.load(os.path.join(scene_path, 'coord.npy')).astype(np.float32)\n",
        "        color = np.load(os.path.join(scene_path, 'color.npy')).astype(np.float32)\n",
        "        normal = np.load(os.path.join(scene_path, 'normal.npy')).astype(np.float32)\n",
        "        gt_segment = np.load(os.path.join(scene_path, 'segment20.npy')).astype(np.int32)\n",
        "        gt_instance = np.load(os.path.join(scene_path, 'instance.npy')).astype(np.int32)\n",
        "\n",
        "        # Store originals\n",
        "        num_points_original = len(coord)\n",
        "\n",
        "        # Prepare data into a dictionary\n",
        "        data_dict = {\n",
        "            'coord': coord,\n",
        "            'color': color,\n",
        "            'normal': normal,\n",
        "            'segment': gt_segment,\n",
        "            'instance': gt_instance,\n",
        "        }\n",
        "\n",
        "\n",
        "        data_dict = transform(data_dict)\n",
        "\n",
        "        #Convert to tensors\n",
        "        for key in data_dict.keys():\n",
        "            if isinstance(data_dict[key], np.ndarray):\n",
        "                if key in ['segment', 'instance', 'grid_coord', 'origin_segment', 'origin_instance']:\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).long()\n",
        "                elif key == 'bbox':\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).long()\n",
        "                else:\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).float()\n",
        "\n",
        "\n",
        "        for key in data_dict.keys():\n",
        "            if isinstance(data_dict[key], torch.Tensor):\n",
        "                data_dict[key] = data_dict[key].cuda()\n",
        "\n",
        "\n",
        "        feat = torch.cat([data_dict[\"coord\"], data_dict[\"color\"], data_dict[\"normal\"]], dim=1)\n",
        "        data_dict[\"feat\"] = feat\n",
        "\n",
        "\n",
        "        num_points = len(data_dict[\"coord\"])\n",
        "        data_dict[\"batch\"] = torch.zeros(num_points, dtype=torch.long).cuda()\n",
        "        data_dict[\"offset\"] = torch.tensor([num_points], dtype=torch.long).cuda()\n",
        "\n",
        "        if \"origin_coord\" in data_dict:\n",
        "            origin_num_points = len(data_dict[\"origin_coord\"])\n",
        "            data_dict[\"origin_offset\"] = torch.tensor([origin_num_points], dtype=torch.long).cuda()\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            output_dict = model(data_dict)\n",
        "\n",
        "        # Get predictions\n",
        "        pred_masks = output_dict['pred_masks']\n",
        "        pred_scores = output_dict['pred_scores']\n",
        "        pred_classes = output_dict['pred_classes']\n",
        "\n",
        "        # Map back to original points if voxelized\n",
        "        if \"origin_coord\" in data_dict and pointops is not None:\n",
        "            reverse, _ = pointops.knn_query(\n",
        "                1,\n",
        "                data_dict[\"coord\"].float(),\n",
        "                data_dict[\"offset\"].int(),\n",
        "                data_dict[\"origin_coord\"].float(),\n",
        "                data_dict[\"origin_offset\"].int(),\n",
        "            )\n",
        "            reverse = reverse.cpu().flatten().long()\n",
        "            pred_masks = pred_masks[:, reverse]\n",
        "\n",
        "        #Converting to numpy\n",
        "        pred_masks = pred_masks.cpu().numpy()\n",
        "        pred_scores = pred_scores.cpu().numpy()\n",
        "        pred_classes = pred_classes.cpu().numpy()\n",
        "\n",
        "        #Make sure predictions match original point cloud size\n",
        "        if pred_masks.shape[1] != num_points_original:\n",
        "            print(f\"\\nWarning: {scene_name} - prediction size mismatch!\")\n",
        "            print(f\"  Original points: {num_points_original}\")\n",
        "            print(f\"  Prediction points: {pred_masks.shape[1]}\")\n",
        "\n",
        "        #Save scene predictions\n",
        "        scene_output = os.path.join(OUTPUT_PATH, 'predictions', f'{scene_name}.npz')\n",
        "        np.savez_compressed(\n",
        "            scene_output,\n",
        "            pred_masks=pred_masks,\n",
        "            pred_scores=pred_scores,\n",
        "            pred_classes=pred_classes,\n",
        "            gt_segment=gt_segment,\n",
        "            gt_instance=gt_instance,\n",
        "            num_points=num_points_original\n",
        "        )\n",
        "\n",
        "        # Update stats\n",
        "        stats['processed'] += 1\n",
        "        stats['total_predictions'] += len(pred_scores)\n",
        "        stats['total_gt_instances'] += len(np.unique(gt_instance[gt_instance >= 0]))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in {scene_name}: {str(e)}\")\n",
        "        stats['failed'] += 1\n",
        "        continue\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'dataset': 'ScanNet validation',\n",
        "    'model': 'SONATA+PointGroup',\n",
        "    'checkpoint': checkpoint_path,\n",
        "    'num_scenes': len(val_scenes),\n",
        "    'processed_scenes': stats['processed'],\n",
        "    'failed_scenes': stats['failed'],\n",
        "    'total_predictions': stats['total_predictions'],\n",
        "    'total_gt_instances': stats['total_gt_instances'],\n",
        "    'avg_predictions_per_scene': stats['total_predictions'] / max(stats['processed'], 1),\n",
        "    'avg_gt_instances_per_scene': stats['total_gt_instances'] / max(stats['processed'], 1),\n",
        "    'output_path': OUTPUT_PATH\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFERENCE COMPLETE\")\n",
        "print(f\"Processed: {stats['processed']}/{len(val_scenes)} scenes\")\n",
        "print(f\"Failed: {stats['failed']} scenes\")\n",
        "print(f\"Average predictions per scene: {summary['avg_predictions_per_scene']:.1f}\")\n",
        "print(f\"Average GT instances per scene: {summary['avg_gt_instances_per_scene']:.1f}\")\n",
        "print(f\"\\nPredictions saved to: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext: Run evaluation script on these predictions\")"
      ],
      "metadata": {
        "id": "tXbNyQdkXmmY",
        "outputId": "0e001a78-dc03-4c66-ee9a-aecf2cb7a8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUN INFERENCE AND SAVE PREDICTIONS WITH POINTGROUP + SONATA\n",
            "============================================================\n",
            "\n",
            "Loading SONATA+PointGroup model...\n",
            "Model loaded\n",
            "\n",
            "Processing 312 validation scenes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running inference: 100%|██████████| 312/312 [32:44<00:00,  6.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INFERENCE COMPLETE\n",
            "Processed: 312/312 scenes\n",
            "Failed: 0 scenes\n",
            "Average predictions per scene: 21.7\n",
            "Average GT instances per scene: 33.2\n",
            "\n",
            "Predictions saved to: /content/drive/MyDrive/Pointcept/scannet_val_evaluation_v1_epoch795_v02\n",
            "============================================================\n",
            "\n",
            "Next: Run evaluation script on these predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pointgroup Evaluator un modified (automatically loads the most recent NPY files). This is where the actual evaluation happens.\n"
      ],
      "metadata": {
        "id": "CyJegYvmXmpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Pointcept')\n",
        "\n",
        "\n",
        "from pointcept.engines.hooks.evaluator import InsSegEvaluator\n",
        "\n",
        "# Configuration\n",
        "PRED_PATH = '/content/drive/MyDrive/Pointcept/scannet_val_evaluation_v1_epoch795_v20'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATING WITH POINTCEPT'S OFFICIAL EVALUATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "#Initialize evaluator\n",
        "evaluator = InsSegEvaluator(\n",
        "    segment_ignore_index=(-1, 0, 1),\n",
        "    instance_ignore_index=-1\n",
        ")\n",
        "\n",
        "# Declare scannet 20 names (excluding wall and floor)\n",
        "class_names = [\n",
        "    \"cabinet\", \"bed\", \"chair\", \"sofa\", \"table\", \"door\",\n",
        "    \"window\", \"bookshelf\", \"picture\", \"counter\", \"desk\",\n",
        "    \"curtain\", \"refrigerator\", \"shower curtain\", \"toilet\",\n",
        "    \"sink\", \"bathtub\", \"otherfurniture\"\n",
        "]\n",
        "evaluator.valid_class_names = class_names\n",
        "\n",
        "#Load all predictions and prepare scenes\n",
        "pred_files = sorted(os.listdir(os.path.join(PRED_PATH, 'predictions')))\n",
        "print(f\"Found {len(pred_files)} prediction files\")\n",
        "\n",
        "scenes = []\n",
        "for pred_file in tqdm(pred_files, desc=\"Processing scenes\"):\n",
        "    scene_name = pred_file.replace('.npz', '')\n",
        "\n",
        "    # Load data\n",
        "    pred_data = np.load(os.path.join(PRED_PATH, 'predictions', pred_file))\n",
        "\n",
        "    #Create prediction dict in the format evaluator expects\n",
        "    pred_dict = {\n",
        "        \"pred_masks\": pred_data['pred_masks'],\n",
        "        \"pred_scores\": pred_data['pred_scores'],\n",
        "        \"pred_classes\": pred_data['pred_classes']\n",
        "    }\n",
        "\n",
        "    #Get ground truth\n",
        "    segment = pred_data['gt_segment']\n",
        "    instance = pred_data['gt_instance']\n",
        "\n",
        "    #Convert to torch tensors for associate_instances\n",
        "    pred_dict_torch = {\n",
        "        \"pred_masks\": torch.from_numpy(pred_dict[\"pred_masks\"]),\n",
        "        \"pred_scores\": torch.from_numpy(pred_dict[\"pred_scores\"]),\n",
        "        \"pred_classes\": torch.from_numpy(pred_dict[\"pred_classes\"])\n",
        "    }\n",
        "    segment_torch = torch.from_numpy(segment)\n",
        "    instance_torch = torch.from_numpy(instance)\n",
        "\n",
        "\n",
        "    class MockTrainer:\n",
        "        def __init__(self):\n",
        "            self.cfg = type('cfg', (), {})()\n",
        "            self.cfg.data = type('data', (), {})()\n",
        "            self.cfg.data.num_classes = 20\n",
        "            self.cfg.data.names = [\n",
        "                \"wall\", \"floor\", \"cabinet\", \"bed\", \"chair\", \"sofa\", \"table\", \"door\",\n",
        "                \"window\", \"bookshelf\", \"picture\", \"counter\", \"desk\", \"curtain\",\n",
        "                \"refrigerator\", \"shower curtain\", \"toilet\", \"sink\", \"bathtub\", \"otherfurniture\"\n",
        "            ]\n",
        "\n",
        "    evaluator.trainer = MockTrainer()\n",
        "\n",
        "\n",
        "    gt_instances, pred_instances = evaluator.associate_instances(\n",
        "        pred_dict_torch, segment_torch, instance_torch\n",
        "    )\n",
        "\n",
        "    # Add to scenes\n",
        "    scenes.append({\n",
        "        \"gt\": gt_instances,\n",
        "        \"pred\": pred_instances\n",
        "    })\n",
        "\n",
        "print(f\"\\nProcessed {len(scenes)} scenes\")\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "#Evaluate matches\n",
        "ap_scores = evaluator.evaluate_matches(scenes)\n",
        "\n",
        "#Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"POINTCEPT EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_ap = ap_scores[\"all_ap\"]\n",
        "all_ap_50 = ap_scores[\"all_ap_50%\"]\n",
        "all_ap_25 = ap_scores[\"all_ap_25%\"]\n",
        "\n",
        "print(f\"\\nOverall Performance:\")\n",
        "print(f\"  mAP: {all_ap:.3f}\")\n",
        "print(f\"  mAP@0.5: {all_ap_50:.3f}\")\n",
        "print(f\"  mAP@0.25: {all_ap_25:.3f}\")\n",
        "\n",
        "print(f\"\\nPer-Class AP@0.5:\")\n",
        "print(f\"{'Class':<20} {'AP':<10} {'AP@0.5':<10} {'AP@0.25':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for label_name in evaluator.valid_class_names:\n",
        "    if label_name in ap_scores[\"classes\"]:\n",
        "        ap = ap_scores[\"classes\"][label_name][\"ap\"]\n",
        "        ap_50 = ap_scores[\"classes\"][label_name][\"ap50%\"]\n",
        "        ap_25 = ap_scores[\"classes\"][label_name][\"ap25%\"]\n",
        "        print(f\"{label_name:<20} {ap:<10.3f} {ap_50:<10.3f} {ap_25:<10.3f}\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRrYXpQVaq-z",
        "outputId": "ff689e20-5aeb-47a6-a451-0a685b2cab98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EVALUATING WITH POINTCEPT'S OFFICIAL EVALUATOR\n",
            "============================================================\n",
            "Found 312 prediction files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing scenes: 100%|██████████| 312/312 [04:24<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed 312 scenes\n",
            "Running evaluation...\n",
            "\n",
            "============================================================\n",
            "POINTCEPT EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Overall Performance:\n",
            "  mAP: 0.423\n",
            "  mAP@0.5: 0.636\n",
            "  mAP@0.25: 0.784\n",
            "\n",
            "Per-Class AP@0.5:\n",
            "Class                AP         AP@0.5     AP@0.25   \n",
            "--------------------------------------------------\n",
            "cabinet              0.327      0.562      0.758     \n",
            "bed                  0.346      0.737      0.836     \n",
            "chair                0.688      0.829      0.889     \n",
            "sofa                 0.345      0.586      0.799     \n",
            "table                0.436      0.684      0.792     \n",
            "door                 0.289      0.532      0.647     \n",
            "window               0.312      0.533      0.773     \n",
            "bookshelf            0.286      0.576      0.753     \n",
            "picture              0.417      0.576      0.668     \n",
            "counter              0.067      0.250      0.645     \n",
            "desk                 0.132      0.387      0.778     \n",
            "curtain              0.351      0.574      0.764     \n",
            "refrigerator         0.529      0.703      0.718     \n",
            "shower curtain       0.628      0.784      0.858     \n",
            "toilet               0.909      1.000      1.000     \n",
            "sink                 0.441      0.693      0.854     \n",
            "bathtub              0.747      0.902      0.903     \n",
            "otherfurniture       0.359      0.540      0.674     \n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: clear gpu cache. If you find that the GPU is filling up and not resetting you can try to clear it with the following snippit"
      ],
      "metadata": {
        "id": "7qqhG14nrxoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Clear GPU CACHE\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Cleared GPU cache after model loading.\")\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# After inference, if you have large intermediate tensors that are no longer needed\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        output_dict = model(data_dict)\n",
        "    print(\"✓ Inference successful\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    # Try Point object approach\n",
        "    try:\n",
        "        from pointcept.models.utils import Point\n",
        "        point = Point(\n",
        "            coord=data_dict.get(\"coord\"),\n",
        "            feat=data_dict.get(\"feat\"),\n",
        "            batch=data_dict.get(\"batch\"),\n",
        "            grid_coord=data_dict.get(\"grid_coord\"),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            output_dict = model(point)\n",
        "        print(\"✓ Inference successful with Point object\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Both approaches failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Cleared GPU cache after inference.\")\n",
        "\n",
        "print(\"\\nModel outputs:\", output_dict.keys())\n",
        "\n"
      ],
      "metadata": {
        "id": "fwSc5UfOWglG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea12a6d-f12a-4bcc-af8e-79a0d6de2e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded SONATA+PointGroup model\n",
            "Cleared GPU cache after model loading.\n",
            "✓ Inference successful\n",
            "Cleared GPU cache after inference.\n",
            "\n",
            "Model outputs: dict_keys(['loss', 'seg_loss', 'bias_l1_loss', 'bias_cosine_loss', 'pred_scores', 'pred_masks', 'pred_classes'])\n"
          ]
        }
      ]
    }
  ]
}