{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/alexmoed/Master_project_Sonata/blob/Eval/eval_v006.ipynb",
      "authorship_tag": "ABX9TyMWeZ2AupLsOm83X7WKF8po",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmoed/MastersProject_Submission/blob/main/Training_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If using colab mount drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "n9GX8cbSuoZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c592619-946e-4adc-d27c-98dbc468cb79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLS"
      ],
      "metadata": {
        "id": "KI7vU5-KBUh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy versions are a ongoing issue in this install it must be below 1.25 to work 1.24.and 1.24.3 are tested and work\n",
        "#Uninstall numpy first\n",
        "#Restart session!!\n",
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y numpy  # Run twice\n",
        "\n",
        "# Find any installs of numpy left\n",
        "!find /usr/local/lib/python*/site-packages -name \"numpy*\" -type d\n",
        "\n",
        "# Delete ALL numpy folders\n",
        "!rm -rf /usr/local/lib/python*/site-packages/numpy*\n",
        "!rm -rf /usr/local/lib/python*/dist-packages/numpy*\n",
        "!rm -rf ~/.local/lib/python*/site-packages/numpy*\n",
        "\n",
        "# Clean pip cache too\n",
        "!pip cache purge\n",
        "\n",
        "# Fresh install\n",
        "!pip install numpy==1.24.0 --no-cache-dir\n",
        "\n",
        "# Make sure the versions are correct and reload if not it needs to be 1.24.0\n",
        "import numpy as np\n",
        "if np.__version__ != \"1.24.0\":\n",
        "   print(f\"Wrong version: {np.__version__}\")\n",
        "\n",
        "\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Numpy location: {np.__file__}\")\n"
      ],
      "metadata": {
        "id": "EiJR_jKXstJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "f0873c05-4b11-4a7c-a9c1-ab968a47b380"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.24.0\n",
            "Uninstalling numpy-1.24.0:\n",
            "  Successfully uninstalled numpy-1.24.0\n",
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mfind: ‘/usr/local/lib/python*/site-packages’: No such file or directory\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Collecting numpy==1.24.0\n",
            "  Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m241.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pywavelets 1.9.0 requires numpy<3,>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "scipy 1.16.1 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.90 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\n",
            "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "treescope 0.1.10 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e63be44da9ac43949a21d13920252c6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version: 1.24.0\n",
            "Numpy location: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import universal dependencies that arent impacted by order\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TA6820kf-QIl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a verison cuda install that after running once that it saves the download and caches the install for speed. Also handles torch installs\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j8OAZS2nBX_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first batch of installs installing cuda and pytorch older versions: This will take 10+ minutes\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_cuda_torch.py').read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qEd_fRE1fy",
        "outputId": "627ae40f-b67b-4885-9921-eeefcd94d1c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "============================================================\n",
            "CUDA INSTALLATION\n",
            "============================================================\n",
            "\n",
            "1. Cleaning existing CUDA installations...\n",
            "✓ Cleaned\n",
            "\n",
            "2. Installing CUDA keyring...\n",
            "✓ Using cached keyring\n",
            "✓ Keyring installed\n",
            "\n",
            "3. Installing CUDA Toolkit 11.8...\n",
            "✓ CUDA 11.8 installed\n",
            "\n",
            "4. Setting environment variables...\n",
            "✓ Environment set\n",
            "\n",
            "5. Verifying installation...\n",
            "\n",
            "✅ CUDA installation complete!\n",
            "Cached files in: /content/drive/MyDrive/colab_cache\n",
            "\n",
            "============================================================\n",
            "PYTORCH INSTALLATION\n",
            "============================================================\n",
            "PyTorch: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "CUDA version: 11.8\n",
            "NumPy: 1.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pointops that is needed for Sonata decoder head. This you have to run twice\n",
        "\n",
        "%cd Pointcept/libs/pointops\n",
        "!pip install -v -e.\n",
        "%cd /content/drive/MyDrive/Pointcept\n"
      ],
      "metadata": {
        "id": "UB-Vs7X6zcDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b25d224-2079-48f7-f583-2b5f7af98f7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Obtaining file:///content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (1.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2025.7.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pointops==1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->pointops==1.0) (1.3.0)\n",
            "Installing collected packages: pointops\n",
            "  Running setup.py develop for pointops\n",
            "    Running command python setup.py develop\n",
            "    running develop\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``easy_install``.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      easy_install.initialize_options(self)\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` directly.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    running egg_info\n",
            "    writing pointops.egg-info/PKG-INFO\n",
            "    writing dependency_links to pointops.egg-info/dependency_links.txt\n",
            "    writing requirements to pointops.egg-info/requires.txt\n",
            "    writing top-level names to pointops.egg-info/top_level.txt\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    reading manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "      warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "    Creating /usr/local/lib/python3.11/dist-packages/pointops.egg-link (link to .)\n",
            "    Adding pointops 1.0 to easy-install.pth file\n",
            "\n",
            "    Installed /content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Successfully installed pointops-1.0\n",
            "/content/drive/MyDrive/Pointcept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Installs\n",
        " %cd /content/drive/MyDrive/Pointcept/Installs\n",
        " exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_build_env.py').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihPYki8jV9Ec",
        "outputId": "9273c5e6-272a-4b0c-da70-17a063d940db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "2.1.0+cu118\n",
            "NumPy: 1.24.0\n",
            "NumPy: 1.24.0\n",
            "============================================================\n",
            "CHECKING ENVIRONMENT\n",
            "============================================================\n",
            "✓ GPU Available: NVIDIA A100-SXM4-40GB\n",
            "✓ CUDA Version: 11.8\n",
            "✓ PyTorch Version: 2.1.0+cu118\n",
            "\n",
            "✓ CUDA Version for installations: cu118\n",
            "\n",
            "============================================================\n",
            "FIXING NUMPY COMPATIBILITY\n",
            "============================================================\n",
            "✓ NumPy 1.24.0 compatibility fixes applied\n",
            "\n",
            "============================================================\n",
            "INSTALLING DEPENDENCIES\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:93: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:93: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:93: FutureWarning: In the future `np.str` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing basic dependencies...\n",
            "\n",
            "Installing PyTorch Geometric dependencies...\n",
            "Installing torch-scatter...\n",
            "Installing torch-sparse...\n",
            "Installing torch-cluster...\n",
            "Installing torch-geometric...\n",
            "\n",
            "Installing spconv...\n",
            "\n",
            "============================================================\n",
            "CLONING POINTCEPT\n",
            "============================================================\n",
            "✓ Cloned Pointcept\n",
            "✓ GPU: NVIDIA A100-SXM4-40GB, Using CUDA architecture: 8.0\n",
            "\n",
            "============================================================\n",
            "BUILDING POINTOPS\n",
            "============================================================\n",
            "Building pointops with pip...\n",
            "✓ pointops imported successfully\n",
            "\n",
            "============================================================\n",
            "BUILDING POINTGROUP_OPS\n",
            "============================================================\n",
            "Building pointgroup_ops with pip...\n",
            "✓ pointgroup_ops imported successfully\n",
            "\n",
            "============================================================\n",
            "FINAL VERIFICATION\n",
            "============================================================\n",
            "✓ torch (2.1.0+cu118)\n",
            "✓ numpy (1.24.0)\n",
            "✓ sklearn (1.3.2)\n",
            "✓ torch_cluster (installed)\n",
            "✓ pointops (installed)\n",
            "✓ pointgroup_ops (installed)\n",
            "✓ spconv (2.3.8)\n",
            "\n",
            "✅ Installation completed successfully!\n",
            "\n",
            "To use Pointcept, add these lines at the start of your code:\n",
            "```python\n",
            "import sys\n",
            "sys.path.insert(0, '/content/Pointcept')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointops')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointgroup_ops')\n",
            "```\n",
            "\n",
            "============================================================\n",
            "QUICK FUNCTIONALITY TEST\n",
            "============================================================\n",
            "✓ Created test tensor on GPU\n",
            "✓ Basic functionality test passed!\n",
            "pointops._C imported successfully!\n",
            "torch-scatter imported successfully!\n",
            "Numpy: 1.24.0\n",
            "Has np.bool: True\n",
            "✅ torch_cluster imported!\n",
            "NumPy version: 1.24.0\n",
            "Patch applied - np.bool is now: <class 'numpy.bool_'>\n",
            "Ops loaded: <built-in method ballquery_batch_p of PyCapsule object at 0x7ea558f5e400> <built-in method bfs_cluster of PyCapsule object at 0x7ea558f5e490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and impliment pointops patches and wrapper\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/fix_ballquery_wrapper.py').read())"
      ],
      "metadata": {
        "id": "pTYh7JZv2SE4",
        "outputId": "3a2b5f15-354e-4010-a9c6-067c6d424a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "FIXING BALLQUERY WRAPPER AFTER TORCH_GEOMETRIC INSTALL\n",
            "================================================================================\n",
            "\n",
            "1. CHECKING ENVIRONMENT AND DEPENDENCIES\n",
            "----------------------------------------\n",
            "  Python version: 3.11.13\n",
            "  ✓ Python version OK\n",
            "  PyTorch version: 2.1.0+cu118\n",
            "  CUDA available: True\n",
            "  CUDA version: 11.8\n",
            "  GPU: NVIDIA A100-SXM4-40GB\n",
            "  PyTorch built with CUDA: 11.8\n",
            "\n",
            "  Checking core dependencies:\n",
            "  ✓ numpy\n",
            "  ✓ plotly\n",
            "  ✓ trimesh\n",
            "  ✓ psutil\n",
            "\n",
            "  ✓ Pointcept found at: /content/Pointcept\n",
            "\n",
            "  Checking pointgroup_ops:\n",
            "  ✓ pointgroup_ops found at: /content/drive/MyDrive/Pointcept/libs/pointgroup_ops/pointgroup_ops.cpython-311-x86_64-linux-gnu.so\n",
            "  ⚠ WARNING: Loading .so file directly\n",
            "  ✓ Has ballquery_batch_p: <class 'builtin_function_or_method'>\n",
            "  ✓ pointgroup_ops_cuda found at: /usr/local/lib/python3.11/dist-packages/pointgroup_ops_cuda.cpython-311-x86_64-linux-gnu.so\n",
            "  ✓ pointops installed (optional)\n",
            "\n",
            "\n",
            "2. FIXING MODEL IMPORTS\n",
            "----------------------------------------\n",
            "\n",
            "  Processing point_group_v1m2_custom_criteria.py...\n",
            "    Created backup: point_group_v1m2_custom_criteria.py.backup_20250814_120009\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "  Processing point_group_v1m1_base.py...\n",
            "    Created backup: point_group_v1m1_base.py.backup_20250814_120009\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "\n",
            "3. FIXING UTILS.PY WRAPPER\n",
            "----------------------------------------\n",
            "  Created backup: utils.py.backup_20250814_120009\n",
            "  Current state:\n",
            "    Has BallQueryBatchP class: True\n",
            "    Has wrapper assignment: True\n",
            "    Has wrapper function: False\n",
            "  ✓ Fixed utils.py wrapper\n",
            "\n",
            "\n",
            "4. CLEARING ALL CACHES\n",
            "----------------------------------------\n",
            "  ✓ Cleared 12 loaded modules from memory\n",
            "\n",
            "\n",
            "5. APPLYING TORCH TYPE COMPATIBILITY FIXES\n",
            "----------------------------------------\n",
            "  ✓ Fixed torch.uint16\n",
            "  ✓ Fixed torch.uint32\n",
            "  ✓ Fixed torch.uint64\n",
            "\n",
            "\n",
            "============================================================\n",
            "FIX COMPLETE\n",
            "============================================================\n",
            "✓ Fixes applied: 5\n",
            "⚠ Warnings: 1\n",
            "  - pointgroup_ops is loading .so directly, not the Python wrapper\n",
            "\n",
            "✅ Ready to import Pointcept and run inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:"
      ],
      "metadata": {
        "id": "8OGn3kitiJTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Required to run any training methods\n",
        "\n",
        "torch_lib_path = os.path.join(os.path.dirname(torch.__file__), 'lib')\n",
        "os.environ['LD_LIBRARY_PATH'] = f\"{torch_lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
        "\n",
        "# Disable wandb for training\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "#set the path\n",
        "%cd /content/drive/MyDrive/Pointcept"
      ],
      "metadata": {
        "id": "5IcFUrVyiUWW",
        "outputId": "a7b248b1-8447-4c9c-9e56-d5cb56797deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segementation training examples:"
      ],
      "metadata": {
        "id": "79X81du4ilwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure it works with scannet 20 dataset\n",
        "!sed -i '/from .scannet import ScanNet3Dataset/d' /content/Pointcept/pointcept/datasets/__init__.py\n",
        "\n",
        "#Full semantic scene training\n",
        "!python tools/train.py \\\n",
        "    --config-file /content/drive/MyDrive/Pointcept/configs/sonata/semseg-sonata-v1m1-0c-scannet-ft-fast.py \\\n",
        "    --num-gpus 1 \\\n",
        "    --options weight=/content/drive/MyDrive/sonata/Checkpoint/pretrain-sonata-v1m1-0-base.pth \\\n",
        "    save_path=exp/sonata/semseg-sonata-v1m1-0c-scannet-ft-_full_scene_v004\n",
        ""
      ],
      "metadata": {
        "id": "ZOOHNW-eiOa-",
        "outputId": "571638d5-9b40-4119-f852-13978229b7bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epochs: 800\n",
            "✅ Loaded config semseg-sonata-v1m1-0c-scannet-ft-fast-a100.py optimized for A100 40GB\n",
            "[2025-08-14 13:29:32,674 INFO train.py line 136 30668] => Loading config ...\n",
            "[2025-08-14 13:29:32,674 INFO train.py line 138 30668] Save path: exp/sonata/semseg-sonata-v1m1-0c-scannet-ft-_full_scene_v004\n",
            "[2025-08-14 13:29:33,323 INFO train.py line 139 30668] Config:\n",
            "weight = '/content/drive/MyDrive/sonata/Checkpoint/pretrain-sonata-v1m1-0-base.pth'\n",
            "resume = False\n",
            "evaluate = False\n",
            "test_only = False\n",
            "seed = 32048753\n",
            "save_path = 'exp/sonata/semseg-sonata-v1m1-0c-scannet-ft-_full_scene_v004'\n",
            "num_worker = 2\n",
            "batch_size = 4\n",
            "batch_size_val = None\n",
            "batch_size_test = None\n",
            "epoch = 800\n",
            "eval_epoch = 800\n",
            "clip_grad = 3.0\n",
            "sync_bn = False\n",
            "enable_amp = True\n",
            "amp_dtype = 'float16'\n",
            "empty_cache = False\n",
            "empty_cache_per_epoch = True\n",
            "find_unused_parameters = False\n",
            "enable_wandb = True\n",
            "wandb_project = 'pointcept'\n",
            "wandb_key = None\n",
            "mix_prob = 0.8\n",
            "param_dicts = [dict(keyword='block', lr=0.0002)]\n",
            "hooks = [\n",
            "    dict(\n",
            "        type='CheckpointLoader',\n",
            "        keywords='module.student.backbone',\n",
            "        replacement='module.backbone'),\n",
            "    dict(type='IterationTimer', warmup_iter=2),\n",
            "    dict(type='InformationWriter'),\n",
            "    dict(type='SemSegEvaluator'),\n",
            "    dict(type='CheckpointSaver', save_freq=5),\n",
            "    dict(type='PreciseEvaluator', test_last=True)\n",
            "]\n",
            "train = dict(type='DefaultTrainer')\n",
            "test = dict(type='SemSegTester', verbose=True)\n",
            "model = dict(\n",
            "    type='DefaultSegmentorV2',\n",
            "    num_classes=20,\n",
            "    backbone_out_channels=64,\n",
            "    backbone=dict(\n",
            "        type='PT-v3m2',\n",
            "        in_channels=9,\n",
            "        order=('z', 'z-trans', 'hilbert', 'hilbert-trans'),\n",
            "        stride=(2, 2, 2, 2),\n",
            "        enc_depths=(3, 3, 3, 12, 3),\n",
            "        enc_channels=(48, 96, 192, 384, 512),\n",
            "        enc_num_head=(3, 6, 12, 24, 32),\n",
            "        enc_patch_size=(1024, 1024, 1024, 1024, 1024),\n",
            "        dec_depths=(2, 2, 2, 2),\n",
            "        dec_channels=(64, 96, 192, 384),\n",
            "        dec_num_head=(4, 6, 12, 24),\n",
            "        dec_patch_size=(1024, 1024, 1024, 1024),\n",
            "        mlp_ratio=4,\n",
            "        qkv_bias=True,\n",
            "        qk_scale=None,\n",
            "        attn_drop=0.0,\n",
            "        proj_drop=0.0,\n",
            "        drop_path=0.3,\n",
            "        shuffle_orders=True,\n",
            "        pre_norm=True,\n",
            "        enable_rpe=False,\n",
            "        enable_flash=False,\n",
            "        upcast_attention=False,\n",
            "        upcast_softmax=False,\n",
            "        traceable=False,\n",
            "        mask_token=False,\n",
            "        enc_mode=False,\n",
            "        freeze_encoder=True),\n",
            "    criteria=[\n",
            "        dict(type='CrossEntropyLoss', loss_weight=1.0, ignore_index=-1),\n",
            "        dict(\n",
            "            type='LovaszLoss',\n",
            "            mode='multiclass',\n",
            "            loss_weight=1.0,\n",
            "            ignore_index=-1)\n",
            "    ],\n",
            "    freeze_backbone=False)\n",
            "optimizer = dict(type='AdamW', lr=0.004, weight_decay=0.02)\n",
            "scheduler = dict(\n",
            "    type='OneCycleLR',\n",
            "    max_lr=[0.002, 0.0002],\n",
            "    pct_start=0.05,\n",
            "    anneal_strategy='cos',\n",
            "    div_factor=10.0,\n",
            "    final_div_factor=1000.0)\n",
            "dataset_type = 'ScanNetDataset'\n",
            "data_root = 'data/scannet'\n",
            "data = dict(\n",
            "    num_classes=20,\n",
            "    ignore_index=-1,\n",
            "    names=[\n",
            "        'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',\n",
            "        'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',\n",
            "        'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',\n",
            "        'otherfurniture'\n",
            "    ],\n",
            "    train=dict(\n",
            "        type='ScanNetDataset',\n",
            "        split='train',\n",
            "        data_root='data/scannet',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(\n",
            "                type='RandomDropout',\n",
            "                dropout_ratio=0.2,\n",
            "                dropout_application_ratio=1.0),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-1, 1],\n",
            "                axis='z',\n",
            "                center=[0, 0, 0],\n",
            "                p=0.5),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-0.015625, 0.015625],\n",
            "                axis='x',\n",
            "                p=0.5),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-0.015625, 0.015625],\n",
            "                axis='y',\n",
            "                p=0.5),\n",
            "            dict(type='RandomScale', scale=[0.9, 1.1]),\n",
            "            dict(type='RandomFlip', p=0.5),\n",
            "            dict(type='RandomJitter', sigma=0.005, clip=0.02),\n",
            "            dict(\n",
            "                type='ElasticDistortion',\n",
            "                distortion_params=[[0.2, 0.4], [0.8, 1.6]]),\n",
            "            dict(type='ChromaticAutoContrast', p=0.2, blend_factor=None),\n",
            "            dict(type='ChromaticTranslation', p=0.95, ratio=0.05),\n",
            "            dict(type='ChromaticJitter', p=0.95, std=0.05),\n",
            "            dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='train',\n",
            "                return_grid_coord=True),\n",
            "            dict(type='SphereCrop', point_max=32768, mode='random'),\n",
            "            dict(type='CenterShift', apply_z=False),\n",
            "            dict(type='NormalizeColor'),\n",
            "            dict(type='ToTensor'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=('coord', 'grid_coord', 'segment'),\n",
            "                feat_keys=('coord', 'color', 'normal'))\n",
            "        ],\n",
            "        test_mode=False,\n",
            "        loop=1),\n",
            "    val=dict(\n",
            "        type='ScanNetDataset',\n",
            "        split='val',\n",
            "        data_root='data/scannet',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(type='Copy', keys_dict=dict(segment='origin_segment')),\n",
            "            dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='train',\n",
            "                return_grid_coord=True,\n",
            "                return_inverse=True),\n",
            "            dict(type='CenterShift', apply_z=False),\n",
            "            dict(type='NormalizeColor'),\n",
            "            dict(type='ToTensor'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=('coord', 'grid_coord', 'segment', 'origin_segment',\n",
            "                      'inverse'),\n",
            "                feat_keys=('coord', 'color', 'normal'))\n",
            "        ],\n",
            "        test_mode=False),\n",
            "    test=dict(\n",
            "        type='ScanNetDataset',\n",
            "        split='val',\n",
            "        data_root='data/scannet',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(type='NormalizeColor')\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        test_cfg=dict(\n",
            "            voxelize=dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='test',\n",
            "                return_grid_coord=True),\n",
            "            crop=None,\n",
            "            post_transform=[\n",
            "                dict(type='CenterShift', apply_z=False),\n",
            "                dict(type='ToTensor'),\n",
            "                dict(\n",
            "                    type='Collect',\n",
            "                    keys=('coord', 'grid_coord', 'index'),\n",
            "                    feat_keys=('coord', 'color', 'normal'))\n",
            "            ],\n",
            "            aug_transform=[[{\n",
            "                'type': 'RandomRotateTargetAngle',\n",
            "                'angle': [0],\n",
            "                'axis': 'z',\n",
            "                'center': [0, 0, 0],\n",
            "                'p': 1\n",
            "            }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }], [{\n",
            "                               'type': 'RandomFlip',\n",
            "                               'p': 1\n",
            "                           }]])))\n",
            "num_worker_per_gpu = 2\n",
            "batch_size_per_gpu = 4\n",
            "batch_size_val_per_gpu = 1\n",
            "batch_size_test_per_gpu = 1\n",
            "\n",
            "[2025-08-14 13:29:33,324 INFO train.py line 140 30668] => Building model ...\n",
            "[2025-08-14 13:29:35,117 INFO train.py line 241 30668] Num params: 16332820\n",
            "[2025-08-14 13:29:35,418 INFO train.py line 142 30668] => Building writer ...\n",
            "[2025-08-14 13:29:35,423 INFO train.py line 251 30668] Tensorboard writer logging dir: exp/sonata/semseg-sonata-v1m1-0c-scannet-ft-_full_scene_v004\n",
            "[2025-08-14 13:29:35,442 INFO train.py line 144 30668] => Building train dataset & dataloader ...\n",
            "[2025-08-14 13:29:35,458 INFO defaults.py line 70 30668] Totally 1201 x 1 samples in scannet train set.\n",
            "[2025-08-14 13:29:35,458 INFO train.py line 146 30668] => Building val dataset & dataloader ...\n",
            "[2025-08-14 13:29:35,458 INFO train.py line 148 30668] => Building optimize, scheduler, scaler(amp) ...\n",
            "[2025-08-14 13:29:35,463 INFO optimizer.py line 56 30668] Params Group 1 - lr: 0.004; Params: ['seg_head.weight', 'seg_head.bias', 'backbone.embedding.stem.linear.weight', 'backbone.embedding.stem.linear.bias', 'backbone.embedding.stem.norm.weight', 'backbone.embedding.stem.norm.bias', 'backbone.enc.enc1.down.proj.weight', 'backbone.enc.enc1.down.proj.bias', 'backbone.enc.enc1.down.norm.0.weight', 'backbone.enc.enc1.down.norm.0.bias', 'backbone.enc.enc2.down.proj.weight', 'backbone.enc.enc2.down.proj.bias', 'backbone.enc.enc2.down.norm.0.weight', 'backbone.enc.enc2.down.norm.0.bias', 'backbone.enc.enc3.down.proj.weight', 'backbone.enc.enc3.down.proj.bias', 'backbone.enc.enc3.down.norm.0.weight', 'backbone.enc.enc3.down.norm.0.bias', 'backbone.enc.enc4.down.proj.weight', 'backbone.enc.enc4.down.proj.bias', 'backbone.enc.enc4.down.norm.0.weight', 'backbone.enc.enc4.down.norm.0.bias', 'backbone.dec.dec3.up.proj.0.weight', 'backbone.dec.dec3.up.proj.0.bias', 'backbone.dec.dec3.up.proj.1.weight', 'backbone.dec.dec3.up.proj.1.bias', 'backbone.dec.dec3.up.proj_skip.0.weight', 'backbone.dec.dec3.up.proj_skip.0.bias', 'backbone.dec.dec3.up.proj_skip.1.weight', 'backbone.dec.dec3.up.proj_skip.1.bias', 'backbone.dec.dec2.up.proj.0.weight', 'backbone.dec.dec2.up.proj.0.bias', 'backbone.dec.dec2.up.proj.1.weight', 'backbone.dec.dec2.up.proj.1.bias', 'backbone.dec.dec2.up.proj_skip.0.weight', 'backbone.dec.dec2.up.proj_skip.0.bias', 'backbone.dec.dec2.up.proj_skip.1.weight', 'backbone.dec.dec2.up.proj_skip.1.bias', 'backbone.dec.dec1.up.proj.0.weight', 'backbone.dec.dec1.up.proj.0.bias', 'backbone.dec.dec1.up.proj.1.weight', 'backbone.dec.dec1.up.proj.1.bias', 'backbone.dec.dec1.up.proj_skip.0.weight', 'backbone.dec.dec1.up.proj_skip.0.bias', 'backbone.dec.dec1.up.proj_skip.1.weight', 'backbone.dec.dec1.up.proj_skip.1.bias', 'backbone.dec.dec0.up.proj.0.weight', 'backbone.dec.dec0.up.proj.0.bias', 'backbone.dec.dec0.up.proj.1.weight', 'backbone.dec.dec0.up.proj.1.bias', 'backbone.dec.dec0.up.proj_skip.0.weight', 'backbone.dec.dec0.up.proj_skip.0.bias', 'backbone.dec.dec0.up.proj_skip.1.weight', 'backbone.dec.dec0.up.proj_skip.1.bias'].\n",
            "[2025-08-14 13:29:35,463 INFO optimizer.py line 56 30668] Params Group 2 - lr: 0.0002; Params: ['backbone.enc.enc0.block0.cpe.0.weight', 'backbone.enc.enc0.block0.cpe.0.bias', 'backbone.enc.enc0.block0.cpe.1.weight', 'backbone.enc.enc0.block0.cpe.1.bias', 'backbone.enc.enc0.block0.cpe.2.weight', 'backbone.enc.enc0.block0.cpe.2.bias', 'backbone.enc.enc0.block0.norm1.0.weight', 'backbone.enc.enc0.block0.norm1.0.bias', 'backbone.enc.enc0.block0.attn.qkv.weight', 'backbone.enc.enc0.block0.attn.qkv.bias', 'backbone.enc.enc0.block0.attn.proj.weight', 'backbone.enc.enc0.block0.attn.proj.bias', 'backbone.enc.enc0.block0.norm2.0.weight', 'backbone.enc.enc0.block0.norm2.0.bias', 'backbone.enc.enc0.block0.mlp.0.fc1.weight', 'backbone.enc.enc0.block0.mlp.0.fc1.bias', 'backbone.enc.enc0.block0.mlp.0.fc2.weight', 'backbone.enc.enc0.block0.mlp.0.fc2.bias', 'backbone.enc.enc0.block1.cpe.0.weight', 'backbone.enc.enc0.block1.cpe.0.bias', 'backbone.enc.enc0.block1.cpe.1.weight', 'backbone.enc.enc0.block1.cpe.1.bias', 'backbone.enc.enc0.block1.cpe.2.weight', 'backbone.enc.enc0.block1.cpe.2.bias', 'backbone.enc.enc0.block1.norm1.0.weight', 'backbone.enc.enc0.block1.norm1.0.bias', 'backbone.enc.enc0.block1.attn.qkv.weight', 'backbone.enc.enc0.block1.attn.qkv.bias', 'backbone.enc.enc0.block1.attn.proj.weight', 'backbone.enc.enc0.block1.attn.proj.bias', 'backbone.enc.enc0.block1.norm2.0.weight', 'backbone.enc.enc0.block1.norm2.0.bias', 'backbone.enc.enc0.block1.mlp.0.fc1.weight', 'backbone.enc.enc0.block1.mlp.0.fc1.bias', 'backbone.enc.enc0.block1.mlp.0.fc2.weight', 'backbone.enc.enc0.block1.mlp.0.fc2.bias', 'backbone.enc.enc0.block2.cpe.0.weight', 'backbone.enc.enc0.block2.cpe.0.bias', 'backbone.enc.enc0.block2.cpe.1.weight', 'backbone.enc.enc0.block2.cpe.1.bias', 'backbone.enc.enc0.block2.cpe.2.weight', 'backbone.enc.enc0.block2.cpe.2.bias', 'backbone.enc.enc0.block2.norm1.0.weight', 'backbone.enc.enc0.block2.norm1.0.bias', 'backbone.enc.enc0.block2.attn.qkv.weight', 'backbone.enc.enc0.block2.attn.qkv.bias', 'backbone.enc.enc0.block2.attn.proj.weight', 'backbone.enc.enc0.block2.attn.proj.bias', 'backbone.enc.enc0.block2.norm2.0.weight', 'backbone.enc.enc0.block2.norm2.0.bias', 'backbone.enc.enc0.block2.mlp.0.fc1.weight', 'backbone.enc.enc0.block2.mlp.0.fc1.bias', 'backbone.enc.enc0.block2.mlp.0.fc2.weight', 'backbone.enc.enc0.block2.mlp.0.fc2.bias', 'backbone.enc.enc1.block0.cpe.0.weight', 'backbone.enc.enc1.block0.cpe.0.bias', 'backbone.enc.enc1.block0.cpe.1.weight', 'backbone.enc.enc1.block0.cpe.1.bias', 'backbone.enc.enc1.block0.cpe.2.weight', 'backbone.enc.enc1.block0.cpe.2.bias', 'backbone.enc.enc1.block0.norm1.0.weight', 'backbone.enc.enc1.block0.norm1.0.bias', 'backbone.enc.enc1.block0.attn.qkv.weight', 'backbone.enc.enc1.block0.attn.qkv.bias', 'backbone.enc.enc1.block0.attn.proj.weight', 'backbone.enc.enc1.block0.attn.proj.bias', 'backbone.enc.enc1.block0.norm2.0.weight', 'backbone.enc.enc1.block0.norm2.0.bias', 'backbone.enc.enc1.block0.mlp.0.fc1.weight', 'backbone.enc.enc1.block0.mlp.0.fc1.bias', 'backbone.enc.enc1.block0.mlp.0.fc2.weight', 'backbone.enc.enc1.block0.mlp.0.fc2.bias', 'backbone.enc.enc1.block1.cpe.0.weight', 'backbone.enc.enc1.block1.cpe.0.bias', 'backbone.enc.enc1.block1.cpe.1.weight', 'backbone.enc.enc1.block1.cpe.1.bias', 'backbone.enc.enc1.block1.cpe.2.weight', 'backbone.enc.enc1.block1.cpe.2.bias', 'backbone.enc.enc1.block1.norm1.0.weight', 'backbone.enc.enc1.block1.norm1.0.bias', 'backbone.enc.enc1.block1.attn.qkv.weight', 'backbone.enc.enc1.block1.attn.qkv.bias', 'backbone.enc.enc1.block1.attn.proj.weight', 'backbone.enc.enc1.block1.attn.proj.bias', 'backbone.enc.enc1.block1.norm2.0.weight', 'backbone.enc.enc1.block1.norm2.0.bias', 'backbone.enc.enc1.block1.mlp.0.fc1.weight', 'backbone.enc.enc1.block1.mlp.0.fc1.bias', 'backbone.enc.enc1.block1.mlp.0.fc2.weight', 'backbone.enc.enc1.block1.mlp.0.fc2.bias', 'backbone.enc.enc1.block2.cpe.0.weight', 'backbone.enc.enc1.block2.cpe.0.bias', 'backbone.enc.enc1.block2.cpe.1.weight', 'backbone.enc.enc1.block2.cpe.1.bias', 'backbone.enc.enc1.block2.cpe.2.weight', 'backbone.enc.enc1.block2.cpe.2.bias', 'backbone.enc.enc1.block2.norm1.0.weight', 'backbone.enc.enc1.block2.norm1.0.bias', 'backbone.enc.enc1.block2.attn.qkv.weight', 'backbone.enc.enc1.block2.attn.qkv.bias', 'backbone.enc.enc1.block2.attn.proj.weight', 'backbone.enc.enc1.block2.attn.proj.bias', 'backbone.enc.enc1.block2.norm2.0.weight', 'backbone.enc.enc1.block2.norm2.0.bias', 'backbone.enc.enc1.block2.mlp.0.fc1.weight', 'backbone.enc.enc1.block2.mlp.0.fc1.bias', 'backbone.enc.enc1.block2.mlp.0.fc2.weight', 'backbone.enc.enc1.block2.mlp.0.fc2.bias', 'backbone.enc.enc2.block0.cpe.0.weight', 'backbone.enc.enc2.block0.cpe.0.bias', 'backbone.enc.enc2.block0.cpe.1.weight', 'backbone.enc.enc2.block0.cpe.1.bias', 'backbone.enc.enc2.block0.cpe.2.weight', 'backbone.enc.enc2.block0.cpe.2.bias', 'backbone.enc.enc2.block0.norm1.0.weight', 'backbone.enc.enc2.block0.norm1.0.bias', 'backbone.enc.enc2.block0.attn.qkv.weight', 'backbone.enc.enc2.block0.attn.qkv.bias', 'backbone.enc.enc2.block0.attn.proj.weight', 'backbone.enc.enc2.block0.attn.proj.bias', 'backbone.enc.enc2.block0.norm2.0.weight', 'backbone.enc.enc2.block0.norm2.0.bias', 'backbone.enc.enc2.block0.mlp.0.fc1.weight', 'backbone.enc.enc2.block0.mlp.0.fc1.bias', 'backbone.enc.enc2.block0.mlp.0.fc2.weight', 'backbone.enc.enc2.block0.mlp.0.fc2.bias', 'backbone.enc.enc2.block1.cpe.0.weight', 'backbone.enc.enc2.block1.cpe.0.bias', 'backbone.enc.enc2.block1.cpe.1.weight', 'backbone.enc.enc2.block1.cpe.1.bias', 'backbone.enc.enc2.block1.cpe.2.weight', 'backbone.enc.enc2.block1.cpe.2.bias', 'backbone.enc.enc2.block1.norm1.0.weight', 'backbone.enc.enc2.block1.norm1.0.bias', 'backbone.enc.enc2.block1.attn.qkv.weight', 'backbone.enc.enc2.block1.attn.qkv.bias', 'backbone.enc.enc2.block1.attn.proj.weight', 'backbone.enc.enc2.block1.attn.proj.bias', 'backbone.enc.enc2.block1.norm2.0.weight', 'backbone.enc.enc2.block1.norm2.0.bias', 'backbone.enc.enc2.block1.mlp.0.fc1.weight', 'backbone.enc.enc2.block1.mlp.0.fc1.bias', 'backbone.enc.enc2.block1.mlp.0.fc2.weight', 'backbone.enc.enc2.block1.mlp.0.fc2.bias', 'backbone.enc.enc2.block2.cpe.0.weight', 'backbone.enc.enc2.block2.cpe.0.bias', 'backbone.enc.enc2.block2.cpe.1.weight', 'backbone.enc.enc2.block2.cpe.1.bias', 'backbone.enc.enc2.block2.cpe.2.weight', 'backbone.enc.enc2.block2.cpe.2.bias', 'backbone.enc.enc2.block2.norm1.0.weight', 'backbone.enc.enc2.block2.norm1.0.bias', 'backbone.enc.enc2.block2.attn.qkv.weight', 'backbone.enc.enc2.block2.attn.qkv.bias', 'backbone.enc.enc2.block2.attn.proj.weight', 'backbone.enc.enc2.block2.attn.proj.bias', 'backbone.enc.enc2.block2.norm2.0.weight', 'backbone.enc.enc2.block2.norm2.0.bias', 'backbone.enc.enc2.block2.mlp.0.fc1.weight', 'backbone.enc.enc2.block2.mlp.0.fc1.bias', 'backbone.enc.enc2.block2.mlp.0.fc2.weight', 'backbone.enc.enc2.block2.mlp.0.fc2.bias', 'backbone.enc.enc3.block0.cpe.0.weight', 'backbone.enc.enc3.block0.cpe.0.bias', 'backbone.enc.enc3.block0.cpe.1.weight', 'backbone.enc.enc3.block0.cpe.1.bias', 'backbone.enc.enc3.block0.cpe.2.weight', 'backbone.enc.enc3.block0.cpe.2.bias', 'backbone.enc.enc3.block0.norm1.0.weight', 'backbone.enc.enc3.block0.norm1.0.bias', 'backbone.enc.enc3.block0.attn.qkv.weight', 'backbone.enc.enc3.block0.attn.qkv.bias', 'backbone.enc.enc3.block0.attn.proj.weight', 'backbone.enc.enc3.block0.attn.proj.bias', 'backbone.enc.enc3.block0.norm2.0.weight', 'backbone.enc.enc3.block0.norm2.0.bias', 'backbone.enc.enc3.block0.mlp.0.fc1.weight', 'backbone.enc.enc3.block0.mlp.0.fc1.bias', 'backbone.enc.enc3.block0.mlp.0.fc2.weight', 'backbone.enc.enc3.block0.mlp.0.fc2.bias', 'backbone.enc.enc3.block1.cpe.0.weight', 'backbone.enc.enc3.block1.cpe.0.bias', 'backbone.enc.enc3.block1.cpe.1.weight', 'backbone.enc.enc3.block1.cpe.1.bias', 'backbone.enc.enc3.block1.cpe.2.weight', 'backbone.enc.enc3.block1.cpe.2.bias', 'backbone.enc.enc3.block1.norm1.0.weight', 'backbone.enc.enc3.block1.norm1.0.bias', 'backbone.enc.enc3.block1.attn.qkv.weight', 'backbone.enc.enc3.block1.attn.qkv.bias', 'backbone.enc.enc3.block1.attn.proj.weight', 'backbone.enc.enc3.block1.attn.proj.bias', 'backbone.enc.enc3.block1.norm2.0.weight', 'backbone.enc.enc3.block1.norm2.0.bias', 'backbone.enc.enc3.block1.mlp.0.fc1.weight', 'backbone.enc.enc3.block1.mlp.0.fc1.bias', 'backbone.enc.enc3.block1.mlp.0.fc2.weight', 'backbone.enc.enc3.block1.mlp.0.fc2.bias', 'backbone.enc.enc3.block2.cpe.0.weight', 'backbone.enc.enc3.block2.cpe.0.bias', 'backbone.enc.enc3.block2.cpe.1.weight', 'backbone.enc.enc3.block2.cpe.1.bias', 'backbone.enc.enc3.block2.cpe.2.weight', 'backbone.enc.enc3.block2.cpe.2.bias', 'backbone.enc.enc3.block2.norm1.0.weight', 'backbone.enc.enc3.block2.norm1.0.bias', 'backbone.enc.enc3.block2.attn.qkv.weight', 'backbone.enc.enc3.block2.attn.qkv.bias', 'backbone.enc.enc3.block2.attn.proj.weight', 'backbone.enc.enc3.block2.attn.proj.bias', 'backbone.enc.enc3.block2.norm2.0.weight', 'backbone.enc.enc3.block2.norm2.0.bias', 'backbone.enc.enc3.block2.mlp.0.fc1.weight', 'backbone.enc.enc3.block2.mlp.0.fc1.bias', 'backbone.enc.enc3.block2.mlp.0.fc2.weight', 'backbone.enc.enc3.block2.mlp.0.fc2.bias', 'backbone.enc.enc3.block3.cpe.0.weight', 'backbone.enc.enc3.block3.cpe.0.bias', 'backbone.enc.enc3.block3.cpe.1.weight', 'backbone.enc.enc3.block3.cpe.1.bias', 'backbone.enc.enc3.block3.cpe.2.weight', 'backbone.enc.enc3.block3.cpe.2.bias', 'backbone.enc.enc3.block3.norm1.0.weight', 'backbone.enc.enc3.block3.norm1.0.bias', 'backbone.enc.enc3.block3.attn.qkv.weight', 'backbone.enc.enc3.block3.attn.qkv.bias', 'backbone.enc.enc3.block3.attn.proj.weight', 'backbone.enc.enc3.block3.attn.proj.bias', 'backbone.enc.enc3.block3.norm2.0.weight', 'backbone.enc.enc3.block3.norm2.0.bias', 'backbone.enc.enc3.block3.mlp.0.fc1.weight', 'backbone.enc.enc3.block3.mlp.0.fc1.bias', 'backbone.enc.enc3.block3.mlp.0.fc2.weight', 'backbone.enc.enc3.block3.mlp.0.fc2.bias', 'backbone.enc.enc3.block4.cpe.0.weight', 'backbone.enc.enc3.block4.cpe.0.bias', 'backbone.enc.enc3.block4.cpe.1.weight', 'backbone.enc.enc3.block4.cpe.1.bias', 'backbone.enc.enc3.block4.cpe.2.weight', 'backbone.enc.enc3.block4.cpe.2.bias', 'backbone.enc.enc3.block4.norm1.0.weight', 'backbone.enc.enc3.block4.norm1.0.bias', 'backbone.enc.enc3.block4.attn.qkv.weight', 'backbone.enc.enc3.block4.attn.qkv.bias', 'backbone.enc.enc3.block4.attn.proj.weight', 'backbone.enc.enc3.block4.attn.proj.bias', 'backbone.enc.enc3.block4.norm2.0.weight', 'backbone.enc.enc3.block4.norm2.0.bias', 'backbone.enc.enc3.block4.mlp.0.fc1.weight', 'backbone.enc.enc3.block4.mlp.0.fc1.bias', 'backbone.enc.enc3.block4.mlp.0.fc2.weight', 'backbone.enc.enc3.block4.mlp.0.fc2.bias', 'backbone.enc.enc3.block5.cpe.0.weight', 'backbone.enc.enc3.block5.cpe.0.bias', 'backbone.enc.enc3.block5.cpe.1.weight', 'backbone.enc.enc3.block5.cpe.1.bias', 'backbone.enc.enc3.block5.cpe.2.weight', 'backbone.enc.enc3.block5.cpe.2.bias', 'backbone.enc.enc3.block5.norm1.0.weight', 'backbone.enc.enc3.block5.norm1.0.bias', 'backbone.enc.enc3.block5.attn.qkv.weight', 'backbone.enc.enc3.block5.attn.qkv.bias', 'backbone.enc.enc3.block5.attn.proj.weight', 'backbone.enc.enc3.block5.attn.proj.bias', 'backbone.enc.enc3.block5.norm2.0.weight', 'backbone.enc.enc3.block5.norm2.0.bias', 'backbone.enc.enc3.block5.mlp.0.fc1.weight', 'backbone.enc.enc3.block5.mlp.0.fc1.bias', 'backbone.enc.enc3.block5.mlp.0.fc2.weight', 'backbone.enc.enc3.block5.mlp.0.fc2.bias', 'backbone.enc.enc3.block6.cpe.0.weight', 'backbone.enc.enc3.block6.cpe.0.bias', 'backbone.enc.enc3.block6.cpe.1.weight', 'backbone.enc.enc3.block6.cpe.1.bias', 'backbone.enc.enc3.block6.cpe.2.weight', 'backbone.enc.enc3.block6.cpe.2.bias', 'backbone.enc.enc3.block6.norm1.0.weight', 'backbone.enc.enc3.block6.norm1.0.bias', 'backbone.enc.enc3.block6.attn.qkv.weight', 'backbone.enc.enc3.block6.attn.qkv.bias', 'backbone.enc.enc3.block6.attn.proj.weight', 'backbone.enc.enc3.block6.attn.proj.bias', 'backbone.enc.enc3.block6.norm2.0.weight', 'backbone.enc.enc3.block6.norm2.0.bias', 'backbone.enc.enc3.block6.mlp.0.fc1.weight', 'backbone.enc.enc3.block6.mlp.0.fc1.bias', 'backbone.enc.enc3.block6.mlp.0.fc2.weight', 'backbone.enc.enc3.block6.mlp.0.fc2.bias', 'backbone.enc.enc3.block7.cpe.0.weight', 'backbone.enc.enc3.block7.cpe.0.bias', 'backbone.enc.enc3.block7.cpe.1.weight', 'backbone.enc.enc3.block7.cpe.1.bias', 'backbone.enc.enc3.block7.cpe.2.weight', 'backbone.enc.enc3.block7.cpe.2.bias', 'backbone.enc.enc3.block7.norm1.0.weight', 'backbone.enc.enc3.block7.norm1.0.bias', 'backbone.enc.enc3.block7.attn.qkv.weight', 'backbone.enc.enc3.block7.attn.qkv.bias', 'backbone.enc.enc3.block7.attn.proj.weight', 'backbone.enc.enc3.block7.attn.proj.bias', 'backbone.enc.enc3.block7.norm2.0.weight', 'backbone.enc.enc3.block7.norm2.0.bias', 'backbone.enc.enc3.block7.mlp.0.fc1.weight', 'backbone.enc.enc3.block7.mlp.0.fc1.bias', 'backbone.enc.enc3.block7.mlp.0.fc2.weight', 'backbone.enc.enc3.block7.mlp.0.fc2.bias', 'backbone.enc.enc3.block8.cpe.0.weight', 'backbone.enc.enc3.block8.cpe.0.bias', 'backbone.enc.enc3.block8.cpe.1.weight', 'backbone.enc.enc3.block8.cpe.1.bias', 'backbone.enc.enc3.block8.cpe.2.weight', 'backbone.enc.enc3.block8.cpe.2.bias', 'backbone.enc.enc3.block8.norm1.0.weight', 'backbone.enc.enc3.block8.norm1.0.bias', 'backbone.enc.enc3.block8.attn.qkv.weight', 'backbone.enc.enc3.block8.attn.qkv.bias', 'backbone.enc.enc3.block8.attn.proj.weight', 'backbone.enc.enc3.block8.attn.proj.bias', 'backbone.enc.enc3.block8.norm2.0.weight', 'backbone.enc.enc3.block8.norm2.0.bias', 'backbone.enc.enc3.block8.mlp.0.fc1.weight', 'backbone.enc.enc3.block8.mlp.0.fc1.bias', 'backbone.enc.enc3.block8.mlp.0.fc2.weight', 'backbone.enc.enc3.block8.mlp.0.fc2.bias', 'backbone.enc.enc3.block9.cpe.0.weight', 'backbone.enc.enc3.block9.cpe.0.bias', 'backbone.enc.enc3.block9.cpe.1.weight', 'backbone.enc.enc3.block9.cpe.1.bias', 'backbone.enc.enc3.block9.cpe.2.weight', 'backbone.enc.enc3.block9.cpe.2.bias', 'backbone.enc.enc3.block9.norm1.0.weight', 'backbone.enc.enc3.block9.norm1.0.bias', 'backbone.enc.enc3.block9.attn.qkv.weight', 'backbone.enc.enc3.block9.attn.qkv.bias', 'backbone.enc.enc3.block9.attn.proj.weight', 'backbone.enc.enc3.block9.attn.proj.bias', 'backbone.enc.enc3.block9.norm2.0.weight', 'backbone.enc.enc3.block9.norm2.0.bias', 'backbone.enc.enc3.block9.mlp.0.fc1.weight', 'backbone.enc.enc3.block9.mlp.0.fc1.bias', 'backbone.enc.enc3.block9.mlp.0.fc2.weight', 'backbone.enc.enc3.block9.mlp.0.fc2.bias', 'backbone.enc.enc3.block10.cpe.0.weight', 'backbone.enc.enc3.block10.cpe.0.bias', 'backbone.enc.enc3.block10.cpe.1.weight', 'backbone.enc.enc3.block10.cpe.1.bias', 'backbone.enc.enc3.block10.cpe.2.weight', 'backbone.enc.enc3.block10.cpe.2.bias', 'backbone.enc.enc3.block10.norm1.0.weight', 'backbone.enc.enc3.block10.norm1.0.bias', 'backbone.enc.enc3.block10.attn.qkv.weight', 'backbone.enc.enc3.block10.attn.qkv.bias', 'backbone.enc.enc3.block10.attn.proj.weight', 'backbone.enc.enc3.block10.attn.proj.bias', 'backbone.enc.enc3.block10.norm2.0.weight', 'backbone.enc.enc3.block10.norm2.0.bias', 'backbone.enc.enc3.block10.mlp.0.fc1.weight', 'backbone.enc.enc3.block10.mlp.0.fc1.bias', 'backbone.enc.enc3.block10.mlp.0.fc2.weight', 'backbone.enc.enc3.block10.mlp.0.fc2.bias', 'backbone.enc.enc3.block11.cpe.0.weight', 'backbone.enc.enc3.block11.cpe.0.bias', 'backbone.enc.enc3.block11.cpe.1.weight', 'backbone.enc.enc3.block11.cpe.1.bias', 'backbone.enc.enc3.block11.cpe.2.weight', 'backbone.enc.enc3.block11.cpe.2.bias', 'backbone.enc.enc3.block11.norm1.0.weight', 'backbone.enc.enc3.block11.norm1.0.bias', 'backbone.enc.enc3.block11.attn.qkv.weight', 'backbone.enc.enc3.block11.attn.qkv.bias', 'backbone.enc.enc3.block11.attn.proj.weight', 'backbone.enc.enc3.block11.attn.proj.bias', 'backbone.enc.enc3.block11.norm2.0.weight', 'backbone.enc.enc3.block11.norm2.0.bias', 'backbone.enc.enc3.block11.mlp.0.fc1.weight', 'backbone.enc.enc3.block11.mlp.0.fc1.bias', 'backbone.enc.enc3.block11.mlp.0.fc2.weight', 'backbone.enc.enc3.block11.mlp.0.fc2.bias', 'backbone.enc.enc4.block0.cpe.0.weight', 'backbone.enc.enc4.block0.cpe.0.bias', 'backbone.enc.enc4.block0.cpe.1.weight', 'backbone.enc.enc4.block0.cpe.1.bias', 'backbone.enc.enc4.block0.cpe.2.weight', 'backbone.enc.enc4.block0.cpe.2.bias', 'backbone.enc.enc4.block0.norm1.0.weight', 'backbone.enc.enc4.block0.norm1.0.bias', 'backbone.enc.enc4.block0.attn.qkv.weight', 'backbone.enc.enc4.block0.attn.qkv.bias', 'backbone.enc.enc4.block0.attn.proj.weight', 'backbone.enc.enc4.block0.attn.proj.bias', 'backbone.enc.enc4.block0.norm2.0.weight', 'backbone.enc.enc4.block0.norm2.0.bias', 'backbone.enc.enc4.block0.mlp.0.fc1.weight', 'backbone.enc.enc4.block0.mlp.0.fc1.bias', 'backbone.enc.enc4.block0.mlp.0.fc2.weight', 'backbone.enc.enc4.block0.mlp.0.fc2.bias', 'backbone.enc.enc4.block1.cpe.0.weight', 'backbone.enc.enc4.block1.cpe.0.bias', 'backbone.enc.enc4.block1.cpe.1.weight', 'backbone.enc.enc4.block1.cpe.1.bias', 'backbone.enc.enc4.block1.cpe.2.weight', 'backbone.enc.enc4.block1.cpe.2.bias', 'backbone.enc.enc4.block1.norm1.0.weight', 'backbone.enc.enc4.block1.norm1.0.bias', 'backbone.enc.enc4.block1.attn.qkv.weight', 'backbone.enc.enc4.block1.attn.qkv.bias', 'backbone.enc.enc4.block1.attn.proj.weight', 'backbone.enc.enc4.block1.attn.proj.bias', 'backbone.enc.enc4.block1.norm2.0.weight', 'backbone.enc.enc4.block1.norm2.0.bias', 'backbone.enc.enc4.block1.mlp.0.fc1.weight', 'backbone.enc.enc4.block1.mlp.0.fc1.bias', 'backbone.enc.enc4.block1.mlp.0.fc2.weight', 'backbone.enc.enc4.block1.mlp.0.fc2.bias', 'backbone.enc.enc4.block2.cpe.0.weight', 'backbone.enc.enc4.block2.cpe.0.bias', 'backbone.enc.enc4.block2.cpe.1.weight', 'backbone.enc.enc4.block2.cpe.1.bias', 'backbone.enc.enc4.block2.cpe.2.weight', 'backbone.enc.enc4.block2.cpe.2.bias', 'backbone.enc.enc4.block2.norm1.0.weight', 'backbone.enc.enc4.block2.norm1.0.bias', 'backbone.enc.enc4.block2.attn.qkv.weight', 'backbone.enc.enc4.block2.attn.qkv.bias', 'backbone.enc.enc4.block2.attn.proj.weight', 'backbone.enc.enc4.block2.attn.proj.bias', 'backbone.enc.enc4.block2.norm2.0.weight', 'backbone.enc.enc4.block2.norm2.0.bias', 'backbone.enc.enc4.block2.mlp.0.fc1.weight', 'backbone.enc.enc4.block2.mlp.0.fc1.bias', 'backbone.enc.enc4.block2.mlp.0.fc2.weight', 'backbone.enc.enc4.block2.mlp.0.fc2.bias', 'backbone.dec.dec3.block0.cpe.0.weight', 'backbone.dec.dec3.block0.cpe.0.bias', 'backbone.dec.dec3.block0.cpe.1.weight', 'backbone.dec.dec3.block0.cpe.1.bias', 'backbone.dec.dec3.block0.cpe.2.weight', 'backbone.dec.dec3.block0.cpe.2.bias', 'backbone.dec.dec3.block0.norm1.0.weight', 'backbone.dec.dec3.block0.norm1.0.bias', 'backbone.dec.dec3.block0.attn.qkv.weight', 'backbone.dec.dec3.block0.attn.qkv.bias', 'backbone.dec.dec3.block0.attn.proj.weight', 'backbone.dec.dec3.block0.attn.proj.bias', 'backbone.dec.dec3.block0.norm2.0.weight', 'backbone.dec.dec3.block0.norm2.0.bias', 'backbone.dec.dec3.block0.mlp.0.fc1.weight', 'backbone.dec.dec3.block0.mlp.0.fc1.bias', 'backbone.dec.dec3.block0.mlp.0.fc2.weight', 'backbone.dec.dec3.block0.mlp.0.fc2.bias', 'backbone.dec.dec3.block1.cpe.0.weight', 'backbone.dec.dec3.block1.cpe.0.bias', 'backbone.dec.dec3.block1.cpe.1.weight', 'backbone.dec.dec3.block1.cpe.1.bias', 'backbone.dec.dec3.block1.cpe.2.weight', 'backbone.dec.dec3.block1.cpe.2.bias', 'backbone.dec.dec3.block1.norm1.0.weight', 'backbone.dec.dec3.block1.norm1.0.bias', 'backbone.dec.dec3.block1.attn.qkv.weight', 'backbone.dec.dec3.block1.attn.qkv.bias', 'backbone.dec.dec3.block1.attn.proj.weight', 'backbone.dec.dec3.block1.attn.proj.bias', 'backbone.dec.dec3.block1.norm2.0.weight', 'backbone.dec.dec3.block1.norm2.0.bias', 'backbone.dec.dec3.block1.mlp.0.fc1.weight', 'backbone.dec.dec3.block1.mlp.0.fc1.bias', 'backbone.dec.dec3.block1.mlp.0.fc2.weight', 'backbone.dec.dec3.block1.mlp.0.fc2.bias', 'backbone.dec.dec2.block0.cpe.0.weight', 'backbone.dec.dec2.block0.cpe.0.bias', 'backbone.dec.dec2.block0.cpe.1.weight', 'backbone.dec.dec2.block0.cpe.1.bias', 'backbone.dec.dec2.block0.cpe.2.weight', 'backbone.dec.dec2.block0.cpe.2.bias', 'backbone.dec.dec2.block0.norm1.0.weight', 'backbone.dec.dec2.block0.norm1.0.bias', 'backbone.dec.dec2.block0.attn.qkv.weight', 'backbone.dec.dec2.block0.attn.qkv.bias', 'backbone.dec.dec2.block0.attn.proj.weight', 'backbone.dec.dec2.block0.attn.proj.bias', 'backbone.dec.dec2.block0.norm2.0.weight', 'backbone.dec.dec2.block0.norm2.0.bias', 'backbone.dec.dec2.block0.mlp.0.fc1.weight', 'backbone.dec.dec2.block0.mlp.0.fc1.bias', 'backbone.dec.dec2.block0.mlp.0.fc2.weight', 'backbone.dec.dec2.block0.mlp.0.fc2.bias', 'backbone.dec.dec2.block1.cpe.0.weight', 'backbone.dec.dec2.block1.cpe.0.bias', 'backbone.dec.dec2.block1.cpe.1.weight', 'backbone.dec.dec2.block1.cpe.1.bias', 'backbone.dec.dec2.block1.cpe.2.weight', 'backbone.dec.dec2.block1.cpe.2.bias', 'backbone.dec.dec2.block1.norm1.0.weight', 'backbone.dec.dec2.block1.norm1.0.bias', 'backbone.dec.dec2.block1.attn.qkv.weight', 'backbone.dec.dec2.block1.attn.qkv.bias', 'backbone.dec.dec2.block1.attn.proj.weight', 'backbone.dec.dec2.block1.attn.proj.bias', 'backbone.dec.dec2.block1.norm2.0.weight', 'backbone.dec.dec2.block1.norm2.0.bias', 'backbone.dec.dec2.block1.mlp.0.fc1.weight', 'backbone.dec.dec2.block1.mlp.0.fc1.bias', 'backbone.dec.dec2.block1.mlp.0.fc2.weight', 'backbone.dec.dec2.block1.mlp.0.fc2.bias', 'backbone.dec.dec1.block0.cpe.0.weight', 'backbone.dec.dec1.block0.cpe.0.bias', 'backbone.dec.dec1.block0.cpe.1.weight', 'backbone.dec.dec1.block0.cpe.1.bias', 'backbone.dec.dec1.block0.cpe.2.weight', 'backbone.dec.dec1.block0.cpe.2.bias', 'backbone.dec.dec1.block0.norm1.0.weight', 'backbone.dec.dec1.block0.norm1.0.bias', 'backbone.dec.dec1.block0.attn.qkv.weight', 'backbone.dec.dec1.block0.attn.qkv.bias', 'backbone.dec.dec1.block0.attn.proj.weight', 'backbone.dec.dec1.block0.attn.proj.bias', 'backbone.dec.dec1.block0.norm2.0.weight', 'backbone.dec.dec1.block0.norm2.0.bias', 'backbone.dec.dec1.block0.mlp.0.fc1.weight', 'backbone.dec.dec1.block0.mlp.0.fc1.bias', 'backbone.dec.dec1.block0.mlp.0.fc2.weight', 'backbone.dec.dec1.block0.mlp.0.fc2.bias', 'backbone.dec.dec1.block1.cpe.0.weight', 'backbone.dec.dec1.block1.cpe.0.bias', 'backbone.dec.dec1.block1.cpe.1.weight', 'backbone.dec.dec1.block1.cpe.1.bias', 'backbone.dec.dec1.block1.cpe.2.weight', 'backbone.dec.dec1.block1.cpe.2.bias', 'backbone.dec.dec1.block1.norm1.0.weight', 'backbone.dec.dec1.block1.norm1.0.bias', 'backbone.dec.dec1.block1.attn.qkv.weight', 'backbone.dec.dec1.block1.attn.qkv.bias', 'backbone.dec.dec1.block1.attn.proj.weight', 'backbone.dec.dec1.block1.attn.proj.bias', 'backbone.dec.dec1.block1.norm2.0.weight', 'backbone.dec.dec1.block1.norm2.0.bias', 'backbone.dec.dec1.block1.mlp.0.fc1.weight', 'backbone.dec.dec1.block1.mlp.0.fc1.bias', 'backbone.dec.dec1.block1.mlp.0.fc2.weight', 'backbone.dec.dec1.block1.mlp.0.fc2.bias', 'backbone.dec.dec0.block0.cpe.0.weight', 'backbone.dec.dec0.block0.cpe.0.bias', 'backbone.dec.dec0.block0.cpe.1.weight', 'backbone.dec.dec0.block0.cpe.1.bias', 'backbone.dec.dec0.block0.cpe.2.weight', 'backbone.dec.dec0.block0.cpe.2.bias', 'backbone.dec.dec0.block0.norm1.0.weight', 'backbone.dec.dec0.block0.norm1.0.bias', 'backbone.dec.dec0.block0.attn.qkv.weight', 'backbone.dec.dec0.block0.attn.qkv.bias', 'backbone.dec.dec0.block0.attn.proj.weight', 'backbone.dec.dec0.block0.attn.proj.bias', 'backbone.dec.dec0.block0.norm2.0.weight', 'backbone.dec.dec0.block0.norm2.0.bias', 'backbone.dec.dec0.block0.mlp.0.fc1.weight', 'backbone.dec.dec0.block0.mlp.0.fc1.bias', 'backbone.dec.dec0.block0.mlp.0.fc2.weight', 'backbone.dec.dec0.block0.mlp.0.fc2.bias', 'backbone.dec.dec0.block1.cpe.0.weight', 'backbone.dec.dec0.block1.cpe.0.bias', 'backbone.dec.dec0.block1.cpe.1.weight', 'backbone.dec.dec0.block1.cpe.1.bias', 'backbone.dec.dec0.block1.cpe.2.weight', 'backbone.dec.dec0.block1.cpe.2.bias', 'backbone.dec.dec0.block1.norm1.0.weight', 'backbone.dec.dec0.block1.norm1.0.bias', 'backbone.dec.dec0.block1.attn.qkv.weight', 'backbone.dec.dec0.block1.attn.qkv.bias', 'backbone.dec.dec0.block1.attn.proj.weight', 'backbone.dec.dec0.block1.attn.proj.bias', 'backbone.dec.dec0.block1.norm2.0.weight', 'backbone.dec.dec0.block1.norm2.0.bias', 'backbone.dec.dec0.block1.mlp.0.fc1.weight', 'backbone.dec.dec0.block1.mlp.0.fc1.bias', 'backbone.dec.dec0.block1.mlp.0.fc2.weight', 'backbone.dec.dec0.block1.mlp.0.fc2.bias'].\n",
            "[2025-08-14 13:29:35,511 INFO train.py line 152 30668] => Building hooks ...\n",
            "[2025-08-14 13:29:35,511 INFO misc.py line 237 30668] => Loading checkpoint & weight ...\n",
            "[2025-08-14 13:29:35,512 INFO misc.py line 239 30668] Loading weight at: /content/drive/MyDrive/sonata/Checkpoint/pretrain-sonata-v1m1-0-base.pth\n",
            "[2025-08-14 13:29:37,776 INFO misc.py line 245 30668] Loading layer weights with keyword: module.student.backbone, replace keyword with: module.backbone\n",
            "[2025-08-14 13:29:37,792 INFO misc.py line 262 30668] Missing keys: ['seg_head.weight', 'seg_head.bias', 'backbone.dec.dec3.up.proj.0.weight', 'backbone.dec.dec3.up.proj.0.bias', 'backbone.dec.dec3.up.proj.1.weight', 'backbone.dec.dec3.up.proj.1.bias', 'backbone.dec.dec3.up.proj_skip.0.weight', 'backbone.dec.dec3.up.proj_skip.0.bias', 'backbone.dec.dec3.up.proj_skip.1.weight', 'backbone.dec.dec3.up.proj_skip.1.bias', 'backbone.dec.dec3.block0.cpe.0.weight', 'backbone.dec.dec3.block0.cpe.0.bias', 'backbone.dec.dec3.block0.cpe.1.weight', 'backbone.dec.dec3.block0.cpe.1.bias', 'backbone.dec.dec3.block0.cpe.2.weight', 'backbone.dec.dec3.block0.cpe.2.bias', 'backbone.dec.dec3.block0.norm1.0.weight', 'backbone.dec.dec3.block0.norm1.0.bias', 'backbone.dec.dec3.block0.attn.qkv.weight', 'backbone.dec.dec3.block0.attn.qkv.bias', 'backbone.dec.dec3.block0.attn.proj.weight', 'backbone.dec.dec3.block0.attn.proj.bias', 'backbone.dec.dec3.block0.norm2.0.weight', 'backbone.dec.dec3.block0.norm2.0.bias', 'backbone.dec.dec3.block0.mlp.0.fc1.weight', 'backbone.dec.dec3.block0.mlp.0.fc1.bias', 'backbone.dec.dec3.block0.mlp.0.fc2.weight', 'backbone.dec.dec3.block0.mlp.0.fc2.bias', 'backbone.dec.dec3.block1.cpe.0.weight', 'backbone.dec.dec3.block1.cpe.0.bias', 'backbone.dec.dec3.block1.cpe.1.weight', 'backbone.dec.dec3.block1.cpe.1.bias', 'backbone.dec.dec3.block1.cpe.2.weight', 'backbone.dec.dec3.block1.cpe.2.bias', 'backbone.dec.dec3.block1.norm1.0.weight', 'backbone.dec.dec3.block1.norm1.0.bias', 'backbone.dec.dec3.block1.attn.qkv.weight', 'backbone.dec.dec3.block1.attn.qkv.bias', 'backbone.dec.dec3.block1.attn.proj.weight', 'backbone.dec.dec3.block1.attn.proj.bias', 'backbone.dec.dec3.block1.norm2.0.weight', 'backbone.dec.dec3.block1.norm2.0.bias', 'backbone.dec.dec3.block1.mlp.0.fc1.weight', 'backbone.dec.dec3.block1.mlp.0.fc1.bias', 'backbone.dec.dec3.block1.mlp.0.fc2.weight', 'backbone.dec.dec3.block1.mlp.0.fc2.bias', 'backbone.dec.dec2.up.proj.0.weight', 'backbone.dec.dec2.up.proj.0.bias', 'backbone.dec.dec2.up.proj.1.weight', 'backbone.dec.dec2.up.proj.1.bias', 'backbone.dec.dec2.up.proj_skip.0.weight', 'backbone.dec.dec2.up.proj_skip.0.bias', 'backbone.dec.dec2.up.proj_skip.1.weight', 'backbone.dec.dec2.up.proj_skip.1.bias', 'backbone.dec.dec2.block0.cpe.0.weight', 'backbone.dec.dec2.block0.cpe.0.bias', 'backbone.dec.dec2.block0.cpe.1.weight', 'backbone.dec.dec2.block0.cpe.1.bias', 'backbone.dec.dec2.block0.cpe.2.weight', 'backbone.dec.dec2.block0.cpe.2.bias', 'backbone.dec.dec2.block0.norm1.0.weight', 'backbone.dec.dec2.block0.norm1.0.bias', 'backbone.dec.dec2.block0.attn.qkv.weight', 'backbone.dec.dec2.block0.attn.qkv.bias', 'backbone.dec.dec2.block0.attn.proj.weight', 'backbone.dec.dec2.block0.attn.proj.bias', 'backbone.dec.dec2.block0.norm2.0.weight', 'backbone.dec.dec2.block0.norm2.0.bias', 'backbone.dec.dec2.block0.mlp.0.fc1.weight', 'backbone.dec.dec2.block0.mlp.0.fc1.bias', 'backbone.dec.dec2.block0.mlp.0.fc2.weight', 'backbone.dec.dec2.block0.mlp.0.fc2.bias', 'backbone.dec.dec2.block1.cpe.0.weight', 'backbone.dec.dec2.block1.cpe.0.bias', 'backbone.dec.dec2.block1.cpe.1.weight', 'backbone.dec.dec2.block1.cpe.1.bias', 'backbone.dec.dec2.block1.cpe.2.weight', 'backbone.dec.dec2.block1.cpe.2.bias', 'backbone.dec.dec2.block1.norm1.0.weight', 'backbone.dec.dec2.block1.norm1.0.bias', 'backbone.dec.dec2.block1.attn.qkv.weight', 'backbone.dec.dec2.block1.attn.qkv.bias', 'backbone.dec.dec2.block1.attn.proj.weight', 'backbone.dec.dec2.block1.attn.proj.bias', 'backbone.dec.dec2.block1.norm2.0.weight', 'backbone.dec.dec2.block1.norm2.0.bias', 'backbone.dec.dec2.block1.mlp.0.fc1.weight', 'backbone.dec.dec2.block1.mlp.0.fc1.bias', 'backbone.dec.dec2.block1.mlp.0.fc2.weight', 'backbone.dec.dec2.block1.mlp.0.fc2.bias', 'backbone.dec.dec1.up.proj.0.weight', 'backbone.dec.dec1.up.proj.0.bias', 'backbone.dec.dec1.up.proj.1.weight', 'backbone.dec.dec1.up.proj.1.bias', 'backbone.dec.dec1.up.proj_skip.0.weight', 'backbone.dec.dec1.up.proj_skip.0.bias', 'backbone.dec.dec1.up.proj_skip.1.weight', 'backbone.dec.dec1.up.proj_skip.1.bias', 'backbone.dec.dec1.block0.cpe.0.weight', 'backbone.dec.dec1.block0.cpe.0.bias', 'backbone.dec.dec1.block0.cpe.1.weight', 'backbone.dec.dec1.block0.cpe.1.bias', 'backbone.dec.dec1.block0.cpe.2.weight', 'backbone.dec.dec1.block0.cpe.2.bias', 'backbone.dec.dec1.block0.norm1.0.weight', 'backbone.dec.dec1.block0.norm1.0.bias', 'backbone.dec.dec1.block0.attn.qkv.weight', 'backbone.dec.dec1.block0.attn.qkv.bias', 'backbone.dec.dec1.block0.attn.proj.weight', 'backbone.dec.dec1.block0.attn.proj.bias', 'backbone.dec.dec1.block0.norm2.0.weight', 'backbone.dec.dec1.block0.norm2.0.bias', 'backbone.dec.dec1.block0.mlp.0.fc1.weight', 'backbone.dec.dec1.block0.mlp.0.fc1.bias', 'backbone.dec.dec1.block0.mlp.0.fc2.weight', 'backbone.dec.dec1.block0.mlp.0.fc2.bias', 'backbone.dec.dec1.block1.cpe.0.weight', 'backbone.dec.dec1.block1.cpe.0.bias', 'backbone.dec.dec1.block1.cpe.1.weight', 'backbone.dec.dec1.block1.cpe.1.bias', 'backbone.dec.dec1.block1.cpe.2.weight', 'backbone.dec.dec1.block1.cpe.2.bias', 'backbone.dec.dec1.block1.norm1.0.weight', 'backbone.dec.dec1.block1.norm1.0.bias', 'backbone.dec.dec1.block1.attn.qkv.weight', 'backbone.dec.dec1.block1.attn.qkv.bias', 'backbone.dec.dec1.block1.attn.proj.weight', 'backbone.dec.dec1.block1.attn.proj.bias', 'backbone.dec.dec1.block1.norm2.0.weight', 'backbone.dec.dec1.block1.norm2.0.bias', 'backbone.dec.dec1.block1.mlp.0.fc1.weight', 'backbone.dec.dec1.block1.mlp.0.fc1.bias', 'backbone.dec.dec1.block1.mlp.0.fc2.weight', 'backbone.dec.dec1.block1.mlp.0.fc2.bias', 'backbone.dec.dec0.up.proj.0.weight', 'backbone.dec.dec0.up.proj.0.bias', 'backbone.dec.dec0.up.proj.1.weight', 'backbone.dec.dec0.up.proj.1.bias', 'backbone.dec.dec0.up.proj_skip.0.weight', 'backbone.dec.dec0.up.proj_skip.0.bias', 'backbone.dec.dec0.up.proj_skip.1.weight', 'backbone.dec.dec0.up.proj_skip.1.bias', 'backbone.dec.dec0.block0.cpe.0.weight', 'backbone.dec.dec0.block0.cpe.0.bias', 'backbone.dec.dec0.block0.cpe.1.weight', 'backbone.dec.dec0.block0.cpe.1.bias', 'backbone.dec.dec0.block0.cpe.2.weight', 'backbone.dec.dec0.block0.cpe.2.bias', 'backbone.dec.dec0.block0.norm1.0.weight', 'backbone.dec.dec0.block0.norm1.0.bias', 'backbone.dec.dec0.block0.attn.qkv.weight', 'backbone.dec.dec0.block0.attn.qkv.bias', 'backbone.dec.dec0.block0.attn.proj.weight', 'backbone.dec.dec0.block0.attn.proj.bias', 'backbone.dec.dec0.block0.norm2.0.weight', 'backbone.dec.dec0.block0.norm2.0.bias', 'backbone.dec.dec0.block0.mlp.0.fc1.weight', 'backbone.dec.dec0.block0.mlp.0.fc1.bias', 'backbone.dec.dec0.block0.mlp.0.fc2.weight', 'backbone.dec.dec0.block0.mlp.0.fc2.bias', 'backbone.dec.dec0.block1.cpe.0.weight', 'backbone.dec.dec0.block1.cpe.0.bias', 'backbone.dec.dec0.block1.cpe.1.weight', 'backbone.dec.dec0.block1.cpe.1.bias', 'backbone.dec.dec0.block1.cpe.2.weight', 'backbone.dec.dec0.block1.cpe.2.bias', 'backbone.dec.dec0.block1.norm1.0.weight', 'backbone.dec.dec0.block1.norm1.0.bias', 'backbone.dec.dec0.block1.attn.qkv.weight', 'backbone.dec.dec0.block1.attn.qkv.bias', 'backbone.dec.dec0.block1.attn.proj.weight', 'backbone.dec.dec0.block1.attn.proj.bias', 'backbone.dec.dec0.block1.norm2.0.weight', 'backbone.dec.dec0.block1.norm2.0.bias', 'backbone.dec.dec0.block1.mlp.0.fc1.weight', 'backbone.dec.dec0.block1.mlp.0.fc1.bias', 'backbone.dec.dec0.block1.mlp.0.fc2.weight', 'backbone.dec.dec0.block1.mlp.0.fc2.bias']\n",
            "[2025-08-14 13:29:37,796 INFO train.py line 159 30668] >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>\n",
            "[2025-08-14 13:29:47,551 INFO misc.py line 117 30668] Train: [1/800][1/300] Data 6.800 (6.800) Batch 9.663 (9.663) Remain 644:11:08 loss: 4.5998 Lr: 0.00020\n",
            "[2025-08-14 13:29:47,902 INFO misc.py line 117 30668] Train: [1/800][2/300] Data 0.003 (0.003) Batch 0.351 (0.351) Remain 23:23:54 loss: 4.6695 Lr: 0.00020\n",
            "[2025-08-14 13:29:53,263 INFO misc.py line 117 30668] Train: [1/800][3/300] Data 4.953 (4.953) Batch 5.361 (5.361) Remain 357:22:34 loss: 4.6206 Lr: 0.00020\n",
            "[2025-08-14 13:29:54,289 INFO misc.py line 117 30668] Train: [1/800][4/300] Data 0.668 (0.668) Batch 1.026 (1.026) Remain 68:25:17 loss: 4.1852 Lr: 0.00020\n",
            "[2025-08-14 13:30:00,828 INFO misc.py line 117 30668] Train: [1/800][5/300] Data 6.183 (3.426) Batch 6.539 (3.783) Remain 252:10:15 loss: 3.6354 Lr: 0.00020\n",
            "[2025-08-14 13:30:01,466 INFO misc.py line 117 30668] Train: [1/800][6/300] Data 0.284 (2.379) Batch 0.638 (2.734) Remain 182:17:31 loss: 3.3261 Lr: 0.00020\n",
            "[2025-08-14 13:30:08,043 INFO misc.py line 117 30668] Train: [1/800][7/300] Data 6.205 (3.335) Batch 6.577 (3.695) Remain 246:19:42 loss: 3.0388 Lr: 0.00020\n",
            "[2025-08-14 13:30:09,016 ERROR events.py line 611 30668] Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/queue.py\", line 180, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Pointcept/pointcept/engines/train.py\", line 168, in train\n",
            "    for (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1284, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "                    ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/queue.py\", line 165, in get\n",
            "    with self.not_empty:\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 274, in __exit__\n",
            "    def __exit__(self, *args):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Floor and wall only"
      ],
      "metadata": {
        "id": "UvmEquCCpeba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remap dataset ScanNet's 20 classes down to just 2 classes saved out npy files (wall and floor)\\"
      ],
      "metadata": {
        "id": "TPK4Iqd1qhgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This remaps the dataset to just have a wall and floor classes\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "data_root = \"/content/drive/MyDrive/Scannet_dataset\"\n",
        "\n",
        "# Process train, val, and test splits\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    split_path = os.path.join(data_root, split)\n",
        "\n",
        "    if not os.path.exists(split_path):\n",
        "        print(f\"\\nSkipping {split} split - not found\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {split} split...\")\n",
        "\n",
        "    #Find all label files (using segment20.npy for 20-class labels)\n",
        "    label_files = glob(os.path.join(data_root, split, \"*\", \"segment20.npy\"))\n",
        "\n",
        "    for label_file in label_files:\n",
        "        # Load labels\n",
        "        labels = np.load(label_file)\n",
        "\n",
        "        #Creating new labels: wall=0, floor=1, everything else=-1\n",
        "        new_labels = np.full_like(labels, -1)\n",
        "        new_labels[labels == 0] = 0  # wall\n",
        "        new_labels[labels == 1] = 1  # floor\n",
        "\n",
        "        # Save as a NEW file\n",
        "        new_file = label_file.replace(\"segment20.npy\", \"segment20_2class.npy\")\n",
        "        np.save(new_file, new_labels)\n",
        "\n",
        "        print(f\"Processed: {os.path.basename(os.path.dirname(label_file))} -> {os.path.basename(new_file)}\")\n",
        "\n",
        "    print(f\"Completed {split} split - {len(label_files)} scenes\")\n",
        "\n",
        "print(\"\\n Done! Now update your config to use 'segment20_2class.npy' instead of 'segment20.npy'\")"
      ],
      "metadata": {
        "id": "KYysGJF_pl2C",
        "outputId": "62ee1275-6d53-434e-f084-bd4d9bb35fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing train split...\n",
            "Processed: scene0107_00 -> segment20_2class.npy\n",
            "Processed: scene0110_02 -> segment20_2class.npy\n",
            "Processed: scene0112_00 -> segment20_2class.npy\n",
            "Processed: scene0106_00 -> segment20_2class.npy\n",
            "Processed: scene0106_01 -> segment20_2class.npy\n",
            "Processed: scene0112_01 -> segment20_2class.npy\n",
            "Processed: scene0112_02 -> segment20_2class.npy\n",
            "Processed: scene0111_01 -> segment20_2class.npy\n",
            "Processed: scene0111_00 -> segment20_2class.npy\n",
            "Processed: scene0113_01 -> segment20_2class.npy\n",
            "Processed: scene0113_00 -> segment20_2class.npy\n",
            "Processed: scene0116_00 -> segment20_2class.npy\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2752656061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Save as a NEW file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnew_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"segment20.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"segment20_2class.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed: {os.path.basename(os.path.dirname(label_file))} -> {os.path.basename(new_file)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training on just the walls and floor\n",
        "!python tools/train.py \\\n",
        "  --config-file /content/drive/MyDrive/Pointcept/configs/sonata/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005.py \\\n",
        "  --num-gpus 1 \\\n",
        "  --options weight=/content/drive/MyDrive/sonata/Checkpoint/pretrain-sonata-v1m1-0-base.pth \\\n",
        "  save_path=exp/scannet/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005 \\\n",
        "  evaluate=False"
      ],
      "metadata": {
        "id": "OJLlP8kHi5lO",
        "outputId": "18701486-440a-4118-d80b-cddc233840e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epochs: 1200\n",
            "✅ Loaded config semseg-sonata-v1m1-0c-scannet3-ft-fast-a100.py for 3-class segmentation\n",
            "[2025-08-14 13:32:31,369 INFO train.py line 136 31547] => Loading config ...\n",
            "[2025-08-14 13:32:31,369 INFO train.py line 138 31547] Save path: exp/scannet/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005\n",
            "[2025-08-14 13:32:32,021 INFO train.py line 139 31547] Config:\n",
            "weight = '/content/drive/MyDrive/sonata/Checkpoint/pretrain-sonata-v1m1-0-base.pth'\n",
            "resume = '/content/drive/MyDrive/Pointcept/exp/scannet/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005/model/epoch_590.pth'\n",
            "evaluate = False\n",
            "test_only = False\n",
            "seed = 31454999\n",
            "save_path = 'exp/scannet/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005'\n",
            "num_worker = 2\n",
            "batch_size = 1\n",
            "batch_size_val = None\n",
            "batch_size_test = None\n",
            "epoch = 1200\n",
            "eval_epoch = 1200\n",
            "clip_grad = 3.0\n",
            "sync_bn = False\n",
            "enable_amp = True\n",
            "amp_dtype = 'float16'\n",
            "empty_cache = False\n",
            "empty_cache_per_epoch = True\n",
            "find_unused_parameters = False\n",
            "enable_wandb = True\n",
            "wandb_project = 'pointcept'\n",
            "wandb_key = None\n",
            "mix_prob = 0.8\n",
            "param_dicts = [dict(keyword='block', lr=0.0001)]\n",
            "hooks = [\n",
            "    dict(\n",
            "        type='CheckpointLoader',\n",
            "        keywords='module.student.backbone',\n",
            "        replacement='module.backbone'),\n",
            "    dict(type='IterationTimer', warmup_iter=2),\n",
            "    dict(type='InformationWriter'),\n",
            "    dict(type='SemSegEvaluator'),\n",
            "    dict(type='CheckpointSaver', save_freq=5),\n",
            "    dict(type='PreciseEvaluator', test_last=True)\n",
            "]\n",
            "train = dict(type='DefaultTrainer')\n",
            "test = dict(type='SemSegTester', verbose=True)\n",
            "model = dict(\n",
            "    type='DefaultSegmentorV2',\n",
            "    num_classes=3,\n",
            "    backbone_out_channels=64,\n",
            "    backbone=dict(\n",
            "        type='PT-v3m2',\n",
            "        in_channels=9,\n",
            "        order=('z', 'z-trans', 'hilbert', 'hilbert-trans'),\n",
            "        stride=(2, 2, 2, 2),\n",
            "        enc_depths=(3, 3, 3, 12, 3),\n",
            "        enc_channels=(48, 96, 192, 384, 512),\n",
            "        enc_num_head=(3, 6, 12, 24, 32),\n",
            "        enc_patch_size=(1024, 1024, 1024, 1024, 1024),\n",
            "        dec_depths=(2, 2, 2, 2),\n",
            "        dec_channels=(64, 96, 192, 384),\n",
            "        dec_num_head=(4, 6, 12, 24),\n",
            "        dec_patch_size=(1024, 1024, 1024, 1024),\n",
            "        mlp_ratio=4,\n",
            "        qkv_bias=True,\n",
            "        qk_scale=None,\n",
            "        attn_drop=0.0,\n",
            "        proj_drop=0.0,\n",
            "        drop_path=0.3,\n",
            "        shuffle_orders=True,\n",
            "        pre_norm=True,\n",
            "        enable_rpe=False,\n",
            "        enable_flash=False,\n",
            "        upcast_attention=False,\n",
            "        upcast_softmax=False,\n",
            "        traceable=False,\n",
            "        mask_token=False,\n",
            "        enc_mode=False,\n",
            "        freeze_encoder=True),\n",
            "    criteria=[\n",
            "        dict(type='CrossEntropyLoss', loss_weight=1.0, ignore_index=-1),\n",
            "        dict(\n",
            "            type='LovaszLoss',\n",
            "            mode='multiclass',\n",
            "            loss_weight=1.0,\n",
            "            ignore_index=-1)\n",
            "    ],\n",
            "    freeze_backbone=False)\n",
            "optimizer = dict(type='AdamW', lr=0.002, weight_decay=0.02)\n",
            "scheduler = dict(\n",
            "    type='OneCycleLR',\n",
            "    max_lr=[0.001, 0.0001],\n",
            "    pct_start=0.05,\n",
            "    anneal_strategy='cos',\n",
            "    div_factor=10.0,\n",
            "    final_div_factor=1000.0)\n",
            "dataset_type = 'ScanNet3Dataset'\n",
            "data_root = '/content/drive/MyDrive/Scannet_dataset'\n",
            "data = dict(\n",
            "    num_classes=3,\n",
            "    ignore_index=-1,\n",
            "    names=['wall', 'floor', 'other'],\n",
            "    train=dict(\n",
            "        type='ScanNet3Dataset',\n",
            "        split='train',\n",
            "        data_root='/content/drive/MyDrive/Scannet_dataset',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(\n",
            "                type='RandomDropout',\n",
            "                dropout_ratio=0.2,\n",
            "                dropout_application_ratio=1.0),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-1, 1],\n",
            "                axis='z',\n",
            "                center=[0, 0, 0],\n",
            "                p=0.5),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-0.015625, 0.015625],\n",
            "                axis='x',\n",
            "                p=0.5),\n",
            "            dict(\n",
            "                type='RandomRotate',\n",
            "                angle=[-0.015625, 0.015625],\n",
            "                axis='y',\n",
            "                p=0.5),\n",
            "            dict(type='RandomScale', scale=[0.9, 1.1]),\n",
            "            dict(type='RandomFlip', p=0.5),\n",
            "            dict(type='RandomJitter', sigma=0.005, clip=0.02),\n",
            "            dict(\n",
            "                type='ElasticDistortion',\n",
            "                distortion_params=[[0.2, 0.4], [0.8, 1.6]]),\n",
            "            dict(type='ChromaticAutoContrast', p=0.2, blend_factor=None),\n",
            "            dict(type='ChromaticTranslation', p=0.95, ratio=0.05),\n",
            "            dict(type='ChromaticJitter', p=0.95, std=0.05),\n",
            "            dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='train',\n",
            "                return_grid_coord=True),\n",
            "            dict(type='SphereCrop', point_max=120768, mode='random'),\n",
            "            dict(type='CenterShift', apply_z=False),\n",
            "            dict(type='NormalizeColor'),\n",
            "            dict(type='ToTensor'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=('coord', 'grid_coord', 'segment'),\n",
            "                feat_keys=('coord', 'color', 'normal'))\n",
            "        ],\n",
            "        test_mode=False,\n",
            "        loop=1),\n",
            "    val=dict(\n",
            "        type='ScanNet3Dataset',\n",
            "        split='val',\n",
            "        data_root='/content/drive/MyDrive/Scannet_dataset',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(type='Copy', keys_dict=dict(segment='origin_segment')),\n",
            "            dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='train',\n",
            "                return_grid_coord=True,\n",
            "                return_inverse=True),\n",
            "            dict(type='CenterShift', apply_z=False),\n",
            "            dict(type='NormalizeColor'),\n",
            "            dict(type='ToTensor'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=('coord', 'grid_coord', 'segment', 'origin_segment',\n",
            "                      'inverse'),\n",
            "                feat_keys=('coord', 'color', 'normal'))\n",
            "        ],\n",
            "        test_mode=False),\n",
            "    test=dict(\n",
            "        type='ScanNet3Dataset',\n",
            "        split='val',\n",
            "        data_root='/content/drive/MyDrive/Scannet_dataset',\n",
            "        transform=[\n",
            "            dict(type='CenterShift', apply_z=True),\n",
            "            dict(type='NormalizeColor')\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        test_cfg=dict(\n",
            "            voxelize=dict(\n",
            "                type='GridSample',\n",
            "                grid_size=0.02,\n",
            "                hash_type='fnv',\n",
            "                mode='test',\n",
            "                return_grid_coord=True),\n",
            "            crop=None,\n",
            "            post_transform=[\n",
            "                dict(type='CenterShift', apply_z=False),\n",
            "                dict(type='ToTensor'),\n",
            "                dict(\n",
            "                    type='Collect',\n",
            "                    keys=('coord', 'grid_coord', 'index'),\n",
            "                    feat_keys=('coord', 'color', 'normal'))\n",
            "            ],\n",
            "            aug_transform=[[{\n",
            "                'type': 'RandomRotateTargetAngle',\n",
            "                'angle': [0],\n",
            "                'axis': 'z',\n",
            "                'center': [0, 0, 0],\n",
            "                'p': 1\n",
            "            }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [0.95, 0.95]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [0.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }],\n",
            "                           [{\n",
            "                               'type': 'RandomRotateTargetAngle',\n",
            "                               'angle': [1.5],\n",
            "                               'axis': 'z',\n",
            "                               'center': [0, 0, 0],\n",
            "                               'p': 1\n",
            "                           }, {\n",
            "                               'type': 'RandomScale',\n",
            "                               'scale': [1.05, 1.05]\n",
            "                           }], [{\n",
            "                               'type': 'RandomFlip',\n",
            "                               'p': 1\n",
            "                           }]])))\n",
            "start_epoch = 591\n",
            "num_worker_per_gpu = 2\n",
            "batch_size_per_gpu = 1\n",
            "batch_size_val_per_gpu = 1\n",
            "batch_size_test_per_gpu = 1\n",
            "\n",
            "[2025-08-14 13:32:32,021 INFO train.py line 140 31547] => Building model ...\n",
            "[2025-08-14 13:32:34,025 INFO train.py line 241 31547] Num params: 16331715\n",
            "[2025-08-14 13:32:34,328 INFO train.py line 142 31547] => Building writer ...\n",
            "[2025-08-14 13:32:34,333 INFO train.py line 251 31547] Tensorboard writer logging dir: exp/scannet/semseg-sonata-v1m1-0c-scannet-ft-_wall_ceiling_v005\n",
            "[2025-08-14 13:32:34,349 INFO train.py line 144 31547] => Building train dataset & dataloader ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Pointcept/pointcept/utils/registry.py\", line 53, in build_from_cfg\n",
            "    return obj_cls(**args)\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/engines/train.py\", line 145, in __init__\n",
            "    self.train_loader = self.build_train_loader()\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/engines/train.py\", line 265, in build_train_loader\n",
            "    train_data = build_dataset(self.cfg.data.train)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/datasets/builder.py\", line 15, in build_dataset\n",
            "    return DATASETS.build(cfg)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/utils/registry.py\", line 214, in build\n",
            "    return self.build_func(*args, **kwargs, registry=self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/utils/registry.py\", line 47, in build_from_cfg\n",
            "    raise KeyError(f\"{obj_type} is not in the {registry.name} registry\")\n",
            "KeyError: 'ScanNet3Dataset is not in the datasets registry'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Pointcept/tools/train.py\", line 38, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/Pointcept/tools/train.py\", line 27, in main\n",
            "    launch(\n",
            "  File \"/content/Pointcept/pointcept/engines/launch.py\", line 89, in launch\n",
            "    main_func(*cfg)\n",
            "  File \"/content/drive/MyDrive/Pointcept/tools/train.py\", line 19, in main_worker\n",
            "    trainer = TRAINERS.build(dict(type=cfg.train.type, cfg=cfg))\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/utils/registry.py\", line 214, in build\n",
            "    return self.build_func(*args, **kwargs, registry=self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pointcept/pointcept/utils/registry.py\", line 56, in build_from_cfg\n",
            "    raise type(e)(f\"{obj_cls.__name__}: {e}\")\n",
            "KeyError: \"Trainer: 'ScanNet3Dataset is not in the datasets registry'\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check what's in the original scannet.py\n",
        "!grep \"^@DATASETS\" -A 2 /content/Pointcept/pointcept/datasets/scannet.py\n",
        "\n",
        "# 2. Check what's in your custom file\n",
        "!grep \"^@DATASETS\" -A 2 /content/drive/MyDrive/Pointcept/pointcept/datasets/Scannet_2_class_remap.py\n",
        "\n",
        "# 3. List all registered datasets\n",
        "from pointcept.datasets.builder import DATASETS\n",
        "print(\"Available datasets:\", list(DATASETS._module_dict.keys()))"
      ],
      "metadata": {
        "id": "U1qPc8ilyQth",
        "outputId": "64c47d07-3694-4a58-9f02-20c455a2d9bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@DATASETS.register_module()\n",
            "class ScanNetDataset(DefaultDataset):\n",
            "    VALID_ASSETS = [\n",
            "--\n",
            "@DATASETS.register_module()\n",
            "class ScanNet200Dataset(ScanNetDataset):\n",
            "    VALID_ASSETS = [\n",
            "@DATASETS.register_module()\n",
            "class ScanNetDataset(DefaultDataset):\n",
            "    VALID_ASSETS = [\n",
            "--\n",
            "@DATASETS.register_module()\n",
            "class ScanNet200Dataset(ScanNetDataset):\n",
            "    VALID_ASSETS = [\n",
            "Available datasets: ['DefaultDataset', 'ConcatDataset', 'S3DISDataset', 'ScanNetDataset', 'ScanNet200Dataset', 'ScanNetPPDataset', 'ScanNetPairDataset', 'HM3DDataset', 'Structured3DDataset', 'AEODataset', 'SemanticKITTIDataset', 'NuScenesDataset', 'WaymoDataset', 'ModelNetDataset', 'ShapeNetPartDataset', 'ScanNet3Dataset']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance Training"
      ],
      "metadata": {
        "id": "ymm_14XLkaW-"
      }
    }
  ]
}