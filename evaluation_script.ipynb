{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/alexmoed/Master_project_Sonata/blob/Eval/eval_v006.ipynb",
      "authorship_tag": "ABX9TyOHivs/leihiVyWI8o1cMWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmoed/MastersProject_Submission/blob/main/evaluation_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If using colab mount drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "n9GX8cbSuoZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c592619-946e-4adc-d27c-98dbc468cb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLS"
      ],
      "metadata": {
        "id": "KI7vU5-KBUh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy versions are a ongoing issue in this install it must be below 1.25 to work 1.24.and 1.24.3 are tested and work\n",
        "#Uninstall numpy first\n",
        "#Restart session!!\n",
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y numpy  # Run twice\n",
        "\n",
        "# Find any installs of numpy left\n",
        "!find /usr/local/lib/python*/site-packages -name \"numpy*\" -type d\n",
        "\n",
        "# Delete ALL numpy folders\n",
        "!rm -rf /usr/local/lib/python*/site-packages/numpy*\n",
        "!rm -rf /usr/local/lib/python*/dist-packages/numpy*\n",
        "!rm -rf ~/.local/lib/python*/site-packages/numpy*\n",
        "\n",
        "# Clean pip cache too\n",
        "!pip cache purge\n",
        "\n",
        "# Fresh install\n",
        "!pip install numpy==1.24.0 --no-cache-dir\n",
        "\n",
        "# Make sure the versions are correct and reload if not it needs to be 1.24.0\n",
        "import numpy as np\n",
        "if np.__version__ != \"1.24.0\":\n",
        "   print(f\"Wrong version: {np.__version__}\")\n",
        "\n",
        "\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Numpy location: {np.__file__}\")\n"
      ],
      "metadata": {
        "id": "EiJR_jKXstJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "f0873c05-4b11-4a7c-a9c1-ab968a47b380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.24.0\n",
            "Uninstalling numpy-1.24.0:\n",
            "  Successfully uninstalled numpy-1.24.0\n",
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mfind: ‘/usr/local/lib/python*/site-packages’: No such file or directory\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Collecting numpy==1.24.0\n",
            "  Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m241.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n",
            "jaxlib 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pywavelets 1.9.0 requires numpy<3,>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "scipy 1.16.1 requires numpy<2.6,>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "chex 0.1.90 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.0 which is incompatible.\n",
            "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "xarray 2025.7.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.0 which is incompatible.\n",
            "arviz 0.22.0 requires numpy>=1.26.0, but you have numpy 1.24.0 which is incompatible.\n",
            "blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.24.0 which is incompatible.\n",
            "treescope 0.1.10 requires numpy>=1.25.2, but you have numpy 1.24.0 which is incompatible.\n",
            "jax 0.5.3 requires numpy>=1.25, but you have numpy 1.24.0 which is incompatible.\n",
            "pymc 5.25.1 requires numpy>=1.25.0, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e63be44da9ac43949a21d13920252c6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy version: 1.24.0\n",
            "Numpy location: /usr/local/lib/python3.11/dist-packages/numpy/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import universal dependencies that arent impacted by order\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TA6820kf-QIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a verison cuda install that after running once that it saves the download and caches the install for speed. Also handles torch installs\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j8OAZS2nBX_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first batch of installs installing cuda and pytorch older versions: This will take 10+ minutes\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_cuda_torch.py').read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qEd_fRE1fy",
        "outputId": "627ae40f-b67b-4885-9921-eeefcd94d1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "============================================================\n",
            "CUDA INSTALLATION\n",
            "============================================================\n",
            "\n",
            "1. Cleaning existing CUDA installations...\n",
            "✓ Cleaned\n",
            "\n",
            "2. Installing CUDA keyring...\n",
            "✓ Using cached keyring\n",
            "✓ Keyring installed\n",
            "\n",
            "3. Installing CUDA Toolkit 11.8...\n",
            "✓ CUDA 11.8 installed\n",
            "\n",
            "4. Setting environment variables...\n",
            "✓ Environment set\n",
            "\n",
            "5. Verifying installation...\n",
            "\n",
            "✅ CUDA installation complete!\n",
            "Cached files in: /content/drive/MyDrive/colab_cache\n",
            "\n",
            "============================================================\n",
            "PYTORCH INSTALLATION\n",
            "============================================================\n",
            "PyTorch: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "CUDA version: 11.8\n",
            "NumPy: 1.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pointops that is needed for Sonata decoder head. This you have to run twice\n",
        "\n",
        "%cd Pointcept/libs/pointops\n",
        "!pip install -v -e.\n",
        "%cd /content/drive/MyDrive/Pointcept\n"
      ],
      "metadata": {
        "id": "UB-Vs7X6zcDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b25d224-2079-48f7-f583-2b5f7af98f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Obtaining file:///content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-l9lu9k0y/pointops.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pointops==1.0) (1.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2025.7.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pointops==1.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pointops==1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->pointops==1.0) (1.3.0)\n",
            "Installing collected packages: pointops\n",
            "  Running setup.py develop for pointops\n",
            "    Running command python setup.py develop\n",
            "    running develop\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``easy_install``.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      easy_install.initialize_options(self)\n",
            "    /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` directly.\n",
            "            Instead, use pypa/build, pypa/installer or other\n",
            "            standards-based tools.\n",
            "\n",
            "            See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    running egg_info\n",
            "    writing pointops.egg-info/PKG-INFO\n",
            "    writing dependency_links to pointops.egg-info/dependency_links.txt\n",
            "    writing requirements to pointops.egg-info/requires.txt\n",
            "    writing top-level names to pointops.egg-info/top_level.txt\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    reading manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'pointops.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n",
            "      warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "    Creating /usr/local/lib/python3.11/dist-packages/pointops.egg-link (link to .)\n",
            "    Adding pointops 1.0 to easy-install.pth file\n",
            "\n",
            "    Installed /content/drive/MyDrive/Pointcept/Pointcept/libs/pointops\n",
            "Successfully installed pointops-1.0\n",
            "/content/drive/MyDrive/Pointcept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Installs\n",
        " %cd /content/drive/MyDrive/Pointcept/Installs\n",
        " exec(open('/content/drive/MyDrive/Pointcept/Installs/setup_build_env.py').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihPYki8jV9Ec",
        "outputId": "9273c5e6-272a-4b0c-da70-17a063d940db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "2.1.0+cu118\n",
            "NumPy: 1.24.0\n",
            "NumPy: 1.24.0\n",
            "============================================================\n",
            "CHECKING ENVIRONMENT\n",
            "============================================================\n",
            "✓ GPU Available: NVIDIA A100-SXM4-40GB\n",
            "✓ CUDA Version: 11.8\n",
            "✓ PyTorch Version: 2.1.0+cu118\n",
            "\n",
            "✓ CUDA Version for installations: cu118\n",
            "\n",
            "============================================================\n",
            "FIXING NUMPY COMPATIBILITY\n",
            "============================================================\n",
            "✓ NumPy 1.24.0 compatibility fixes applied\n",
            "\n",
            "============================================================\n",
            "INSTALLING DEPENDENCIES\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:93: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:93: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
            "<string>:93: FutureWarning: In the future `np.str` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing basic dependencies...\n",
            "\n",
            "Installing PyTorch Geometric dependencies...\n",
            "Installing torch-scatter...\n",
            "Installing torch-sparse...\n",
            "Installing torch-cluster...\n",
            "Installing torch-geometric...\n",
            "\n",
            "Installing spconv...\n",
            "\n",
            "============================================================\n",
            "CLONING POINTCEPT\n",
            "============================================================\n",
            "✓ Cloned Pointcept\n",
            "✓ GPU: NVIDIA A100-SXM4-40GB, Using CUDA architecture: 8.0\n",
            "\n",
            "============================================================\n",
            "BUILDING POINTOPS\n",
            "============================================================\n",
            "Building pointops with pip...\n",
            "✓ pointops imported successfully\n",
            "\n",
            "============================================================\n",
            "BUILDING POINTGROUP_OPS\n",
            "============================================================\n",
            "Building pointgroup_ops with pip...\n",
            "✓ pointgroup_ops imported successfully\n",
            "\n",
            "============================================================\n",
            "FINAL VERIFICATION\n",
            "============================================================\n",
            "✓ torch (2.1.0+cu118)\n",
            "✓ numpy (1.24.0)\n",
            "✓ sklearn (1.3.2)\n",
            "✓ torch_cluster (installed)\n",
            "✓ pointops (installed)\n",
            "✓ pointgroup_ops (installed)\n",
            "✓ spconv (2.3.8)\n",
            "\n",
            "✅ Installation completed successfully!\n",
            "\n",
            "To use Pointcept, add these lines at the start of your code:\n",
            "```python\n",
            "import sys\n",
            "sys.path.insert(0, '/content/Pointcept')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointops')\n",
            "sys.path.insert(0, '/content/Pointcept/libs/pointgroup_ops')\n",
            "```\n",
            "\n",
            "============================================================\n",
            "QUICK FUNCTIONALITY TEST\n",
            "============================================================\n",
            "✓ Created test tensor on GPU\n",
            "✓ Basic functionality test passed!\n",
            "pointops._C imported successfully!\n",
            "torch-scatter imported successfully!\n",
            "Numpy: 1.24.0\n",
            "Has np.bool: True\n",
            "✅ torch_cluster imported!\n",
            "NumPy version: 1.24.0\n",
            "Patch applied - np.bool is now: <class 'numpy.bool_'>\n",
            "Ops loaded: <built-in method ballquery_batch_p of PyCapsule object at 0x7ea558f5e400> <built-in method bfs_cluster of PyCapsule object at 0x7ea558f5e490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and impliment pointops patches and wrapper\n",
        "%cd /content/drive/MyDrive/Pointcept/Installs\n",
        "exec(open('/content/drive/MyDrive/Pointcept/Installs/fix_ballquery_wrapper.py').read())"
      ],
      "metadata": {
        "id": "pTYh7JZv2SE4",
        "outputId": "3a2b5f15-354e-4010-a9c6-067c6d424a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pointcept/Installs\n",
            "FIXING BALLQUERY WRAPPER AFTER TORCH_GEOMETRIC INSTALL\n",
            "================================================================================\n",
            "\n",
            "1. CHECKING ENVIRONMENT AND DEPENDENCIES\n",
            "----------------------------------------\n",
            "  Python version: 3.11.13\n",
            "  ✓ Python version OK\n",
            "  PyTorch version: 2.1.0+cu118\n",
            "  CUDA available: True\n",
            "  CUDA version: 11.8\n",
            "  GPU: NVIDIA A100-SXM4-40GB\n",
            "  PyTorch built with CUDA: 11.8\n",
            "\n",
            "  Checking core dependencies:\n",
            "  ✓ numpy\n",
            "  ✓ plotly\n",
            "  ✓ trimesh\n",
            "  ✓ psutil\n",
            "\n",
            "  ✓ Pointcept found at: /content/Pointcept\n",
            "\n",
            "  Checking pointgroup_ops:\n",
            "  ✓ pointgroup_ops found at: /content/drive/MyDrive/Pointcept/libs/pointgroup_ops/pointgroup_ops.cpython-311-x86_64-linux-gnu.so\n",
            "  ⚠ WARNING: Loading .so file directly\n",
            "  ✓ Has ballquery_batch_p: <class 'builtin_function_or_method'>\n",
            "  ✓ pointgroup_ops_cuda found at: /usr/local/lib/python3.11/dist-packages/pointgroup_ops_cuda.cpython-311-x86_64-linux-gnu.so\n",
            "  ✓ pointops installed (optional)\n",
            "\n",
            "\n",
            "2. FIXING MODEL IMPORTS\n",
            "----------------------------------------\n",
            "\n",
            "  Processing point_group_v1m2_custom_criteria.py...\n",
            "    Created backup: point_group_v1m2_custom_criteria.py.backup_20250814_120009\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "  Processing point_group_v1m1_base.py...\n",
            "    Created backup: point_group_v1m1_base.py.backup_20250814_120009\n",
            "    ✓ Fixed successfully\n",
            "\n",
            "\n",
            "3. FIXING UTILS.PY WRAPPER\n",
            "----------------------------------------\n",
            "  Created backup: utils.py.backup_20250814_120009\n",
            "  Current state:\n",
            "    Has BallQueryBatchP class: True\n",
            "    Has wrapper assignment: True\n",
            "    Has wrapper function: False\n",
            "  ✓ Fixed utils.py wrapper\n",
            "\n",
            "\n",
            "4. CLEARING ALL CACHES\n",
            "----------------------------------------\n",
            "  ✓ Cleared 12 loaded modules from memory\n",
            "\n",
            "\n",
            "5. APPLYING TORCH TYPE COMPATIBILITY FIXES\n",
            "----------------------------------------\n",
            "  ✓ Fixed torch.uint16\n",
            "  ✓ Fixed torch.uint32\n",
            "  ✓ Fixed torch.uint64\n",
            "\n",
            "\n",
            "============================================================\n",
            "FIX COMPLETE\n",
            "============================================================\n",
            "✓ Fixes applied: 5\n",
            "⚠ Warnings: 1\n",
            "  - pointgroup_ops is loading .so directly, not the Python wrapper\n",
            "\n",
            "✅ Ready to import Pointcept and run inference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following runs evaluations using Sonta and pointgroup and saves the predictions are NPZ files."
      ],
      "metadata": {
        "id": "54LGRqUGYTLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your config and checkpoint paths here:\n",
        "SCANNET_VAL_PATH = '/content/drive/MyDrive/Scannet_dataset/val'\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/Pointcept/scannet_val_evaluation_v1_epoch795_v02' #best epoch 465\n",
        "checkpoint_path = '/content/drive/MyDrive/Pointcept/exp/scannet/insseg-pointgroup-sonata_config_4/model/epoch_795.pth'\n",
        "\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_PATH, 'predictions'), exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RUN INFERENCE AND SAVE PREDICTIONS WITH POINTGROUP + SONATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nLoading SONATA+PointGroup model...\")\n",
        "cfg = Config.fromfile('/content/drive/MyDrive/Pointcept/Pointcept/configs/scannet/insseg-pointgroup-v1m2-0-ptv3-base_2.py')\n",
        "model = build_model(cfg.model).cuda()\n",
        "model.eval()\n",
        "\n",
        "#Loading checkpoint here\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "weight = OrderedDict()\n",
        "for key, value in checkpoint[\"state_dict\"].items():\n",
        "    if key.startswith(\"module.\"):\n",
        "        key = key[7:]\n",
        "    weight[key] = value\n",
        "model.load_state_dict(weight, strict=True)\n",
        "print(\"Model loaded\")\n",
        "\n",
        "\n",
        "transform_list = [\n",
        "    dict(type=\"CenterShift\", apply_z=True),\n",
        "    dict(\n",
        "        type=\"Copy\",\n",
        "        keys_dict={\n",
        "            \"coord\": \"origin_coord\",\n",
        "            \"segment\": \"origin_segment\",\n",
        "            \"instance\": \"origin_instance\",\n",
        "        },\n",
        "    ),\n",
        "    dict(\n",
        "        type=\"GridSample\",\n",
        "        grid_size=0.0175, #Experiment with different gridsizes usually from .01 - .025  smaller the size the more detail and vram useage\n",
        "        hash_type=\"fnv\",\n",
        "        mode=\"train\",\n",
        "        return_grid_coord=True,\n",
        "    ),\n",
        "    dict(type=\"CenterShift\", apply_z=False),\n",
        "    dict(type=\"NormalizeColor\"),\n",
        "    dict(\n",
        "        type=\"InstanceParser\",\n",
        "        segment_ignore_index=(-1, 0, 1),\n",
        "        instance_ignore_index=-1,\n",
        "    ),\n",
        "]\n",
        "transform = Compose(transform_list)\n",
        "\n",
        "#Load validation scenes\n",
        "val_scenes = sorted([d for d in os.listdir(SCANNET_VAL_PATH) if os.path.isdir(os.path.join(SCANNET_VAL_PATH, d))])\n",
        "print(f\"\\nProcessing {len(val_scenes)} validation scenes...\")\n",
        "\n",
        "# Track stats\n",
        "stats = {\n",
        "    'processed': 0,\n",
        "    'failed': 0,\n",
        "    'total_predictions': 0,\n",
        "    'total_gt_instances': 0\n",
        "}\n",
        "\n",
        "# Process each scene in folder\n",
        "for scene_name in tqdm(val_scenes, desc=\"Running inference\"):\n",
        "    scene_path = os.path.join(SCANNET_VAL_PATH, scene_name)\n",
        "\n",
        "    try:\n",
        "        #Load scene data\n",
        "        coord = np.load(os.path.join(scene_path, 'coord.npy')).astype(np.float32)\n",
        "        color = np.load(os.path.join(scene_path, 'color.npy')).astype(np.float32)\n",
        "        normal = np.load(os.path.join(scene_path, 'normal.npy')).astype(np.float32)\n",
        "        gt_segment = np.load(os.path.join(scene_path, 'segment20.npy')).astype(np.int32)\n",
        "        gt_instance = np.load(os.path.join(scene_path, 'instance.npy')).astype(np.int32)\n",
        "\n",
        "        # Store originals\n",
        "        num_points_original = len(coord)\n",
        "\n",
        "        # Prepare data\n",
        "        data_dict = {\n",
        "            'coord': coord,\n",
        "            'color': color,\n",
        "            'normal': normal,\n",
        "            'segment': gt_segment,\n",
        "            'instance': gt_instance,\n",
        "        }\n",
        "\n",
        "        # Apply transforms\n",
        "        data_dict = transform(data_dict)\n",
        "\n",
        "        # Convert to tensors\n",
        "        for key in data_dict.keys():\n",
        "            if isinstance(data_dict[key], np.ndarray):\n",
        "                if key in ['segment', 'instance', 'grid_coord', 'origin_segment', 'origin_instance']:\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).long()\n",
        "                elif key == 'bbox':\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).long()\n",
        "                else:\n",
        "                    data_dict[key] = torch.from_numpy(data_dict[key]).float()\n",
        "\n",
        "        # Move to CUDA\n",
        "        for key in data_dict.keys():\n",
        "            if isinstance(data_dict[key], torch.Tensor):\n",
        "                data_dict[key] = data_dict[key].cuda()\n",
        "\n",
        "        # Create features\n",
        "        feat = torch.cat([data_dict[\"coord\"], data_dict[\"color\"], data_dict[\"normal\"]], dim=1)\n",
        "        data_dict[\"feat\"] = feat\n",
        "\n",
        "        # Add batch info\n",
        "        num_points = len(data_dict[\"coord\"])\n",
        "        data_dict[\"batch\"] = torch.zeros(num_points, dtype=torch.long).cuda()\n",
        "        data_dict[\"offset\"] = torch.tensor([num_points], dtype=torch.long).cuda()\n",
        "\n",
        "        if \"origin_coord\" in data_dict:\n",
        "            origin_num_points = len(data_dict[\"origin_coord\"])\n",
        "            data_dict[\"origin_offset\"] = torch.tensor([origin_num_points], dtype=torch.long).cuda()\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            output_dict = model(data_dict)\n",
        "\n",
        "        # Get predictions\n",
        "        pred_masks = output_dict['pred_masks']\n",
        "        pred_scores = output_dict['pred_scores']\n",
        "        pred_classes = output_dict['pred_classes']\n",
        "\n",
        "        # Map back to original points if voxelized\n",
        "        if \"origin_coord\" in data_dict and pointops is not None:\n",
        "            reverse, _ = pointops.knn_query(\n",
        "                1,\n",
        "                data_dict[\"coord\"].float(),\n",
        "                data_dict[\"offset\"].int(),\n",
        "                data_dict[\"origin_coord\"].float(),\n",
        "                data_dict[\"origin_offset\"].int(),\n",
        "            )\n",
        "            reverse = reverse.cpu().flatten().long()\n",
        "            pred_masks = pred_masks[:, reverse]\n",
        "\n",
        "        #Converting to numpy\n",
        "        pred_masks = pred_masks.cpu().numpy()\n",
        "        pred_scores = pred_scores.cpu().numpy()\n",
        "        pred_classes = pred_classes.cpu().numpy()\n",
        "\n",
        "        #Make sure predictions match original point cloud size\n",
        "        if pred_masks.shape[1] != num_points_original:\n",
        "            print(f\"\\nWarning: {scene_name} - prediction size mismatch!\")\n",
        "            print(f\"  Original points: {num_points_original}\")\n",
        "            print(f\"  Prediction points: {pred_masks.shape[1]}\")\n",
        "\n",
        "        #Save scene predictions\n",
        "        scene_output = os.path.join(OUTPUT_PATH, 'predictions', f'{scene_name}.npz')\n",
        "        np.savez_compressed(\n",
        "            scene_output,\n",
        "            pred_masks=pred_masks,\n",
        "            pred_scores=pred_scores,\n",
        "            pred_classes=pred_classes,\n",
        "            gt_segment=gt_segment,\n",
        "            gt_instance=gt_instance,\n",
        "            num_points=num_points_original\n",
        "        )\n",
        "\n",
        "        # Update statistics\n",
        "        stats['processed'] += 1\n",
        "        stats['total_predictions'] += len(pred_scores)\n",
        "        stats['total_gt_instances'] += len(np.unique(gt_instance[gt_instance >= 0]))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in {scene_name}: {str(e)}\")\n",
        "        stats['failed'] += 1\n",
        "        continue\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'dataset': 'ScanNet validation',\n",
        "    'model': 'SONATA+PointGroup',\n",
        "    'checkpoint': checkpoint_path,\n",
        "    'num_scenes': len(val_scenes),\n",
        "    'processed_scenes': stats['processed'],\n",
        "    'failed_scenes': stats['failed'],\n",
        "    'total_predictions': stats['total_predictions'],\n",
        "    'total_gt_instances': stats['total_gt_instances'],\n",
        "    'avg_predictions_per_scene': stats['total_predictions'] / max(stats['processed'], 1),\n",
        "    'avg_gt_instances_per_scene': stats['total_gt_instances'] / max(stats['processed'], 1),\n",
        "    'output_path': OUTPUT_PATH\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_PATH, 'inference_summary.json'), 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFERENCE COMPLETE\")\n",
        "print(f\"Processed: {stats['processed']}/{len(val_scenes)} scenes\")\n",
        "print(f\"Failed: {stats['failed']} scenes\")\n",
        "print(f\"Average predictions per scene: {summary['avg_predictions_per_scene']:.1f}\")\n",
        "print(f\"Average GT instances per scene: {summary['avg_gt_instances_per_scene']:.1f}\")\n",
        "print(f\"\\nPredictions saved to: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext: Run evaluation script on these predictions\")"
      ],
      "metadata": {
        "id": "tXbNyQdkXmmY",
        "outputId": "a1a01ec6-c750-45d5-f9a8-8541ae3f85c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUN INFERENCE AND SAVE PREDICTIONS WITH POINTGROUP + SONATA\n",
            "============================================================\n",
            "\n",
            "Loading SONATA+PointGroup model...\n",
            "Model loaded\n",
            "\n",
            "Processing 312 validation scenes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running inference:   9%|▊         | 27/312 [01:38<17:19,  3.65s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2009948809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/point_group/point_group_v1m2_custom_criteria.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_dict, return_point)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"offset\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;34m\"pooling_parent\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m2_sonata.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparsify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Point module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPointModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Spconv module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mspconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_spconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Point module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPointModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Spconv module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mspconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_spconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m2_sonata.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, point)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortcut\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m2_sonata.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, point)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# ffn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pointcept Evaluator un modified (automatically loads the most recent NPY files).\n"
      ],
      "metadata": {
        "id": "CyJegYvmXmpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Pointcept')\n",
        "\n",
        "\n",
        "from pointcept.engines.hooks.evaluator import InsSegEvaluator\n",
        "\n",
        "# Configuration\n",
        "PRED_PATH = '/content/drive/MyDrive/Pointcept/scannet_val_evaluation_v1_epoch795_v20'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATING WITH POINTCEPT'S OFFICIAL EVALUATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "#Initialize evaluator\n",
        "evaluator = InsSegEvaluator(\n",
        "    segment_ignore_index=(-1, 0, 1),\n",
        "    instance_ignore_index=-1\n",
        ")\n",
        "\n",
        "# Declare scannet 20 names (excluding wall and floor)\n",
        "class_names = [\n",
        "    \"cabinet\", \"bed\", \"chair\", \"sofa\", \"table\", \"door\",\n",
        "    \"window\", \"bookshelf\", \"picture\", \"counter\", \"desk\",\n",
        "    \"curtain\", \"refrigerator\", \"shower curtain\", \"toilet\",\n",
        "    \"sink\", \"bathtub\", \"otherfurniture\"\n",
        "]\n",
        "evaluator.valid_class_names = class_names\n",
        "\n",
        "# Load all predictions and prepare scenes\n",
        "pred_files = sorted(os.listdir(os.path.join(PRED_PATH, 'predictions')))\n",
        "print(f\"Found {len(pred_files)} prediction files\")\n",
        "\n",
        "scenes = []\n",
        "for pred_file in tqdm(pred_files, desc=\"Processing scenes\"):\n",
        "    scene_name = pred_file.replace('.npz', '')\n",
        "\n",
        "    # Load data\n",
        "    pred_data = np.load(os.path.join(PRED_PATH, 'predictions', pred_file))\n",
        "\n",
        "    # Create prediction dict in the format evaluator expects\n",
        "    pred_dict = {\n",
        "        \"pred_masks\": pred_data['pred_masks'],\n",
        "        \"pred_scores\": pred_data['pred_scores'],\n",
        "        \"pred_classes\": pred_data['pred_classes']\n",
        "    }\n",
        "\n",
        "    # Get ground truth\n",
        "    segment = pred_data['gt_segment']\n",
        "    instance = pred_data['gt_instance']\n",
        "\n",
        "    # Convert to torch tensors for associate_instances\n",
        "    pred_dict_torch = {\n",
        "        \"pred_masks\": torch.from_numpy(pred_dict[\"pred_masks\"]),\n",
        "        \"pred_scores\": torch.from_numpy(pred_dict[\"pred_scores\"]),\n",
        "        \"pred_classes\": torch.from_numpy(pred_dict[\"pred_classes\"])\n",
        "    }\n",
        "    segment_torch = torch.from_numpy(segment)\n",
        "    instance_torch = torch.from_numpy(instance)\n",
        "\n",
        "\n",
        "    class MockTrainer:\n",
        "        def __init__(self):\n",
        "            self.cfg = type('cfg', (), {})()\n",
        "            self.cfg.data = type('data', (), {})()\n",
        "            self.cfg.data.num_classes = 20\n",
        "            self.cfg.data.names = [\n",
        "                \"wall\", \"floor\", \"cabinet\", \"bed\", \"chair\", \"sofa\", \"table\", \"door\",\n",
        "                \"window\", \"bookshelf\", \"picture\", \"counter\", \"desk\", \"curtain\",\n",
        "                \"refrigerator\", \"shower curtain\", \"toilet\", \"sink\", \"bathtub\", \"otherfurniture\"\n",
        "            ]\n",
        "\n",
        "    evaluator.trainer = MockTrainer()\n",
        "\n",
        "    # Associate instances\n",
        "    gt_instances, pred_instances = evaluator.associate_instances(\n",
        "        pred_dict_torch, segment_torch, instance_torch\n",
        "    )\n",
        "\n",
        "    # Add to scenes\n",
        "    scenes.append({\n",
        "        \"gt\": gt_instances,\n",
        "        \"pred\": pred_instances\n",
        "    })\n",
        "\n",
        "print(f\"\\nProcessed {len(scenes)} scenes\")\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "# Evaluate matches\n",
        "ap_scores = evaluator.evaluate_matches(scenes)\n",
        "\n",
        "#Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"POINTCEPT EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_ap = ap_scores[\"all_ap\"]\n",
        "all_ap_50 = ap_scores[\"all_ap_50%\"]\n",
        "all_ap_25 = ap_scores[\"all_ap_25%\"]\n",
        "\n",
        "print(f\"\\nOverall Performance:\")\n",
        "print(f\"  mAP: {all_ap:.3f}\")\n",
        "print(f\"  mAP@0.5: {all_ap_50:.3f}\")\n",
        "print(f\"  mAP@0.25: {all_ap_25:.3f}\")\n",
        "\n",
        "print(f\"\\nPer-Class AP@0.5:\")\n",
        "print(f\"{'Class':<20} {'AP':<10} {'AP@0.5':<10} {'AP@0.25':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for label_name in evaluator.valid_class_names:\n",
        "    if label_name in ap_scores[\"classes\"]:\n",
        "        ap = ap_scores[\"classes\"][label_name][\"ap\"]\n",
        "        ap_50 = ap_scores[\"classes\"][label_name][\"ap50%\"]\n",
        "        ap_25 = ap_scores[\"classes\"][label_name][\"ap25%\"]\n",
        "        print(f\"{label_name:<20} {ap:<10.3f} {ap_50:<10.3f} {ap_25:<10.3f}\")\n",
        "\n",
        "#Save results for future use to Json file\n",
        "output_file = os.path.join(PRED_PATH, 'pointcept_evaluation_results.json')\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(ap_scores, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Results saved to: {output_file}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "RRrYXpQVaq-z",
        "outputId": "61a6742b-6ae6-479f-929f-185135d82746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EVALUATING WITH POINTCEPT'S OFFICIAL EVALUATOR\n",
            "============================================================\n",
            "Found 312 prediction files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing scenes:   3%|▎         | 8/312 [00:04<03:07,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3192561374.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Get ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0msegment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_segment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_instance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    821\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    825\u001b[0m                                                              count=read_count)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: clear gpu cache. If you find that the GPU is filling up and not resetting you can try to clear it with the following snippit"
      ],
      "metadata": {
        "id": "7qqhG14nrxoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Clear GPU CACHE\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Cleared GPU cache after model loading.\")\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# After inference, if you have large intermediate tensors that are no longer needed\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        output_dict = model(data_dict)\n",
        "    print(\"✓ Inference successful\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    # Try Point object approach\n",
        "    try:\n",
        "        from pointcept.models.utils import Point\n",
        "        point = Point(\n",
        "            coord=data_dict.get(\"coord\"),\n",
        "            feat=data_dict.get(\"feat\"),\n",
        "            batch=data_dict.get(\"batch\"),\n",
        "            grid_coord=data_dict.get(\"grid_coord\"),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            output_dict = model(point)\n",
        "        print(\"✓ Inference successful with Point object\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Both approaches failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Cleared GPU cache after inference.\")\n",
        "\n",
        "print(\"\\nModel outputs:\", output_dict.keys())\n",
        "\n"
      ],
      "metadata": {
        "id": "fwSc5UfOWglG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea12a6d-f12a-4bcc-af8e-79a0d6de2e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded SONATA+PointGroup model\n",
            "Cleared GPU cache after model loading.\n",
            "✓ Inference successful\n",
            "Cleared GPU cache after inference.\n",
            "\n",
            "Model outputs: dict_keys(['loss', 'seg_loss', 'bias_l1_loss', 'bias_cosine_loss', 'pred_scores', 'pred_masks', 'pred_classes'])\n"
          ]
        }
      ]
    }
  ]
}